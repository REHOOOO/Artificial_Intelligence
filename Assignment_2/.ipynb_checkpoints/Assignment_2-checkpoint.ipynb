{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb75138",
   "metadata": {},
   "source": [
    "# 강화학습 구현과 실현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f575fc67",
   "metadata": {},
   "source": [
    "강화학습에는 여러 기법이 있는데 여기에서는 DQN(Deep Q-Network)를 사용하여 강화학습을 진행하였습니다  \n",
    "DQN은 기존의 Q-Learning에 신경망을 결합한 것입니다  \n",
    "Q-Learning은 주어진 상태에서 행동을 수행하면서 미래의 효율적인 기댓값을 예측하는 Q 함수를 학습하면서 최적의 정책을 학습히는 기법입니다  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64c041",
   "metadata": {},
   "source": [
    "## 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61256157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ad440",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b5bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531a439",
   "metadata": {},
   "source": [
    "## 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb14248",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1  # 랜덤하게 행동할 확률\n",
    "epsilonMinimumValue = 0.001  # epsilon의 최소값\n",
    "nbActions = 3  # 행동의 개수 (왼쪽, 대기, 오른쪽)\n",
    "epoch = 1001  # 게임 반복횟수\n",
    "hiddenSize = 100  # 히든 레이어 뉴런 개수\n",
    "maxMemory = 500  # 게임내용을 기억하는 최대 개수\n",
    "batchSize = 50  # 학습시 데이터 묶음 개수\n",
    "gridSize = 10  # 격자 크기\n",
    "nbStates = gridSize * gridSize  # 상태 개수 (픽셀의 개수)\n",
    "discount = 0.9  # 감소값\n",
    "learningRate = 0.2  # 학습률"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3545422f",
   "metadata": {},
   "source": [
    "## 딥러닝 모델 설정\n",
    "입력레이어는 nbStates(100)개 히든레이어는 hiddenSize(100)개 출력레이어는 nbAction(3)개를 가지는 딥러닝 모델을 만들어 준다  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5d7238",
   "metadata": {},
   "source": [
    "### 입력 레이어\n",
    "**X = tf.compat.v1.placeholder(tf.float32, [None, nbStates]) # 입력값**  \n",
    "placeholder는 변수를 선언할 때 값을 바로 주는 것이 아닌 나중에 값을 던져줄 수 있도록 공간을 미리 만들어 주는 것이다  \n",
    "따라서 X는 데이터 유형이 float32이고 첫번째 차원의 수는 정해져있지 않고(가변적) 두번째 차원의 수는 nbStates의 값을 가지는 placeholder가 된다  \n",
    "\n",
    "**W1 = tf.Variable(tf.random.truncated_normal([nbStates, hiddenSize], stddev=1.0 / math.sqrt(float(nbStates)))) # 가중치**  \n",
    "입력 레이어의 가중치를 나타내는 W1을 생성해 준다  \n",
    "변수의 초기화는 랜덤값으로 주어지는데 이 랜덤값은 양쪽 끝이 잘려있는 정규분포에서 가져오게 된다\n",
    "이 정규분포의 표준편차는 $\\frac{1}{\\sqrt{nbStates}}$가 된다  \n",
    "이러한 랜덤값들로 nbStates x hiddenSize의 크기를 가지는 행렬을 채워준다  \n",
    "\n",
    "**b1 = tf.Variable(tf.random.truncated_normal([hiddenSize], stddev=0.01)) # 편향**  \n",
    "활성화 난이도를 조절해주는 b1을 생성해준다  \n",
    "학습데이터가 가중치와 계산되어 나온 값에 더해주어 활성화 난이도를 조절해주는 역할을 해준다  \n",
    "여기서도 양쪽 끝이 잘려있는 정규분포를 이용하게 되는데 이 정규분포의 표준편차는 0.01이 된다  \n",
    "hiddenSize개의 랜덤값들을 만들어준다  \n",
    "\n",
    "**input_layer = tf.nn.relu(tf.matmul(X, W1) + b1)**  \n",
    "input layer의 출력값을 만들어준다  \n",
    "우선 X과 W1의 행렬곱을 한 다음 b1을 더해준다  \n",
    "그 다음 활성화 함수에 넣어주는데 여기서 활성화 함수는 ReLU 함수가 사용된다  \n",
    "ReLU 함수: $f(x) = max(0,x)$  \n",
    "\n",
    "입력값인 X와 가중치인 W1을 행렬곱을 하게 되면 X는 None x nbStates의 크기를 가지는 행렬이고 W1은 nbStates x hiddenSize의 크기를 가지는 행렬이므로 결과는 None x hiddenSize의 크기를 가지는 행렬이 나오게 된다  \n",
    "이렇게 행렬곱을 하게되면 각각의 입력 레이어 노드에 들어가는 입력값들을 만들어낼 수 있다  \n",
    "이 값에 편향인 b1을 더해준 뒤 활성화 함수에 넣어주면 각각의 입력 레이어 노드의 출력값들을 만들어낼 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43bef925",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.compat.v1.placeholder(tf.float32, [None, nbStates]) # 입력값\n",
    "W1 = tf.Variable(tf.random.truncated_normal([nbStates, hiddenSize], stddev=1.0 / math.sqrt(float(nbStates)))) # 가중치\n",
    "b1 = tf.Variable(tf.random.truncated_normal([hiddenSize], stddev=0.01)) # 편향\n",
    "input_layer = tf.nn.relu(tf.matmul(X, W1) + b1) # 출력값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c081ec08",
   "metadata": {},
   "source": [
    "### 히든 레이어\n",
    "**W2 = tf.Variable(tf.random.truncated_normal([hiddenSize, hiddenSize],stddev=1.0 / math.sqrt(float(hiddenSize))))**  \n",
    "히든 레이어의 가중치를 나타내는 W2를 생성해준다\n",
    "입력 레이어와 마찬가지로 변수의 초기화는 랜덤값으로 주어지는데 이 랜덤값은 양쪽 끝이 잘려있는 정규분포에서 가져오게 된다  \n",
    "이 정규분포의 표준편차는 $\\frac{1}{\\sqrt{hiddenSize}}$가 된다  \n",
    "이러한 랜덤값들로 hiddenSize x hiddenSize의 크기를 가지는 행렬을 채워준다  \n",
    "\n",
    "**b2 = tf.Variable(tf.random.truncated_normal([hiddenSize], stddev=0.01)) # 편향**  \n",
    "활성화 난이도를 조절해주는 b2을 생성해준다  \n",
    "입력 레이어와 마찬가지로 학습데이터가 가중치와 계산되어 나온 값에 더해주어 활성화 난이도를 조절해주는 역할을 해준다  \n",
    "여기서도 양쪽 끝이 잘려있는 정규분포를 이용하게 되는데 이 정규분포의 표준편차는 0.01이 된다  \n",
    "hiddenSize개의 랜덤값들을 만들어준다  \n",
    "\n",
    "**hidden_layer = tf.nn.relu(tf.matmul(input_layer, W2) + b2) # 출력값**  \n",
    "hidden layer의 출력값을 만들어준다  \n",
    "우선 input_layer(입력 레이어의 출력값)과 W2의 행렬곱을 한 다음 b2을 더해준다  \n",
    "그 다음 활성화 함수에 넣어주는데 여기서도 활성화 함수는 ReLU 함수가 사용된다  \n",
    "ReLU 함수: $f(x) = max(0,x)$  \n",
    "\n",
    "입력 레이어의 출력값인 input_layer와 가중치인 W2을 행렬곱을 하게 되면 input_layer는 None x hiddenSize 크기를 가지는 행렬이고 W2은 hiddenSize x hiddenSize의 크기를 가지는 행렬이므로 결과는 None x hiddenSize의 크기를 가지는 행렬이 나오게 된다  \n",
    "이렇게 행렬곱을 하게되면 각각의 히든 레이어 노드에 들어가는 입력값들을 만들어낼 수 있다  \n",
    "이 값에 편향인 b2을 더해준 뒤 활성화 함수에 넣어주면 각각의 히든 레이어 노드의 출력값들을 만들어낼 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "726af6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.random.truncated_normal([hiddenSize, hiddenSize],stddev=1.0 / math.sqrt(float(hiddenSize)))) # 가중치\n",
    "b2 = tf.Variable(tf.random.truncated_normal([hiddenSize], stddev=0.01)) # 편향\n",
    "hidden_layer = tf.nn.relu(tf.matmul(input_layer, W2) + b2) # 출력값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83214b22",
   "metadata": {},
   "source": [
    "### 출력 레이어  \n",
    "**W3 = tf.Variable(tf.random.truncated_normal([hiddenSize, nbActions],stddev=1.0 / math.sqrt(float(hiddenSize))))**  \n",
    "출력 레이어의 가중치를 나타내는 W3을 생성해준다  \n",
    "히든 레이어와 마찬가지로 변수의 초기화는 랜덤값으로 주어지는데 이 랜덤값은 양쪽 끝이 잘려있는 정규분포에서 가져오게 된다  \n",
    "이 정규분포의 표준편차는 $\\frac{1}{\\sqrt{hiddenSize}}$가 된다  \n",
    "이러한 랜덤값들로 hiddenSize x hiddenSize의 크기를 가지는 행렬을 채워준다  \n",
    "\n",
    "**b3 = tf.Variable(tf.random.truncated_normal([nbActions], stddev=0.01))**  \n",
    "활성화 난이도를 조절해주는 b3을 생성해준다  \n",
    "히든 레이어와 마찬가지로 학습데이터가 가중치와 계산되어 나온 값에 더해주어 활성화 난이도를 조절해주는 역할을 해준다  \n",
    "여기서도 양쪽 끝이 잘려있는 정규분포를 이용하게 되는데 이 정규분포의 표준편차는 0.01이 된다  \n",
    "bnActions개의 랜덤값들을 만들어준다   \n",
    "\n",
    "**output_layer = tf.matmul(hidden_layer, W3) + b3**  \n",
    "out layer의 출력값을 만들어준다\n",
    "hidden_layer(히든레이어의 출력값)과 W3의 행렬곱을 한 다음 b3을 더해준다  \n",
    "회귀 문제이기 때문에 출력 레이어에서는 활성화 함수를 사용하지 않습니다 (항등함수라고도 함)\n",
    "\n",
    "히든 레이어의 출력값인 hidden_layer와 가중치인 W3을 행렬곱을 하게 되면 hidden_layer는 None x hiddenSize 크기를 가지는 행렬이고 W3은 hiddenSize x nbActions 크기를 가지는 행렬이므로 결과는 None x nbActions 크기를 가지는 행렬이 나오게 된다  \n",
    "이렇게 행렬곱을 하게되면 각각의 출력 레이어 노드에 들어가는 입력값들을 만들어낼 수 있다  \n",
    "이 값에 편향인 b3을 더해주면 각각의 출력 레이어 노드의 출력값들을 만들어낼 수 있다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a7fa893",
   "metadata": {},
   "outputs": [],
   "source": [
    "W3 = tf.Variable(tf.random.truncated_normal([hiddenSize, nbActions],stddev=1.0 / math.sqrt(float(hiddenSize))))\n",
    "b3 = tf.Variable(tf.random.truncated_normal([nbActions], stddev=0.01))\n",
    "output_layer = tf.matmul(hidden_layer, W3) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a38d445",
   "metadata": {},
   "source": [
    "### 목표값 플레이스홀더\n",
    "목표값 플레이스홀더 Y를 생성한다  \n",
    "Y는 데이터 유형이 float32이고 첫번째 차원의 수는 정해져있지 않고(가변적) 두번째 차원의 수는 nbActions의 값을 가지는 placeholder가 된다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faff8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.compat.v1.placeholder(tf.float32, [None, nbActions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc48f32",
   "metadata": {},
   "source": [
    "### 목표값과 출력값의 차이인 코스트\n",
    "목표값과 출력값의 오차를 구하기 위해 여기서는 평균 제곱 오차를 사용한다  \n",
    "목표값(Y)에서 출력값(output_layer)을 뺀 다음 제곱을 해준다  \n",
    "그 다음 나온 값들을 모두 더해준 뒤 2 x batchSize로 나누어준다  \n",
    "batchSize는 한 번에 모델이 학습하는 데이터 샘플의 개수로 이렇게 2 x batchSize로 나누어주게 되면 코스트를 정규화해주게 된다  \n",
    "정규화를 통해 학습률과 batchSize에 따른 코스트 크기의 영향을 줄일 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d4a5486",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_sum(tf.square(Y-output_layer)) / (2*batchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864aea12",
   "metadata": {},
   "source": [
    "### 경사하강법으로 코스트가 최소가 되는 값 찾음\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learningRate).minimize(cost)  \n",
    "경사하강법을 이용해 비용이 최소가 되는 값을 찾아준다  \n",
    "경사하강법을 사용할 때 학습률은 learningRate가 되고 minimize(cost)를 붙여주어 cost를 최소화하도록 설정해준다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c6b9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learningRate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c59ca",
   "metadata": {},
   "source": [
    "## 랜덤값 함수\n",
    "함수 randf는 s이상 e미만의 값을 가지는 랜덤값을 리턴해준다  \n",
    "(float(random.randrange(0, (e - s) * 9999)) / 10000) + s;\n",
    "random.randrange는 0부터 (e - s) * 9999 미만의 랜덤값을 만들어준다  \n",
    "이 값에 float를 붙여 실수로 변환해준 다음 10000으로 나누어주면 0이상 (e - s)미만의 랜덤값이 나오게 된다  \n",
    "여기에 s를 더해주면 s이상 e미만의 랜덤값이 된다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfdf95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randf(s, e):\n",
    "    return (float(random.randrange(0, (e - s) * 9999)) / 10000) + s;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277ba72",
   "metadata": {},
   "source": [
    "## 환경 클래스\n",
    "### \\_\\_init__ 함수\n",
    "\\_\\_init__ 함수는 초기화를 시켜준다  \n",
    "\n",
    "**self.gridSize = gridSize  \n",
    "self.nbStates = self.gridSize * self.gridSize  \n",
    "self.state = np.empty(3, dtype = np.uint8)**  \n",
    "np.empty는 초기화되지 않은 값으로 배열을 생성해주는 함수로 여기서는 길이가 3인 초기화되지 않은 배열이 생성된다  \n",
    "배열의 데이터 형식은 uint8(부호 없는 8비트 정수)가 된다  \n",
    "\n",
    "### observe 함수\n",
    "observe 함수는 화면정보를 리턴해준다  \n",
    "\n",
    "**canvas = self.drawState()**  \n",
    "drawState함수의 출력값을 canvas에 저장해준다  \n",
    "\n",
    "**canvas = np.reshape(canvas, (-1,self.nbStates))**  \n",
    "np.reshape는 배열을 재구성해주는 함수로 여기서는 canvas 배열을 self.nbStates개의 열을 가지는 배열로 재구성해준다(행의 개수는 -1로 설정되어 있으므로 자동으로 결정된다)  \n",
    "그 값을 다시 canvas에 저장해준다    \n",
    "\n",
    "return canvas  \n",
    "canvas를 리턴해준다  \n",
    "\n",
    "### drawState 함수\n",
    "drawState 함수는 블럭과 바를 표시하여 화면정보를 리턴해준다\n",
    "\n",
    "**canvas = np.zeros((self.gridSize, self.gridSize))**  \n",
    "모든 요소가 0으로 초기화된 self.gridSize x self.gridSize의 크기를 가지는 배열을 생성한다  \n",
    "\n",
    "**canvas[self.state[0]-1, self.state[1]-1] = 1**  \n",
    "self.state는 길이가 3인 배열로 0에는 블럭의 행번호가 1에는 블럭의 열번호를 저장하고 있다  \n",
    "self.state 값들을 계산할때는 gridSize를 기준으로 계산을 해 1 ~ gridSize이지만 실제로 나타낼 때는 0 ~ gridSize-1이기 때문에 각 값들에 -1을 해준 위치의 배열값을 1로 바꾸어준다   \n",
    "\n",
    "**canvas[self.gridSize-1, self.state[2] -1 - 1] = 1  \n",
    "canvas[self.gridSize-1, self.state[2] -1] = 1  \n",
    "canvas[self.gridSize-1, self.state[2] -1 + 1] = 1**  \n",
    "self.State[2]에는 바의 열번호를 저장하고 있다  \n",
    "따라서 바의 행번호는 self.gridSize-1(가장 마지막 행)을 해주고 열번호는 self.state[2]-1과 좌우 배열값들을 1로 바꾸어준다    \n",
    "\n",
    "return canvas  \n",
    "canvas를 리턴해준다  \n",
    "\n",
    "### reset 함수\n",
    "reset함수는 블럭과 바의 위치를 리셋해준다  \n",
    "\n",
    "**initialFruitColumn = random.randrange(1, self.gridSize + 1)**  \n",
    "1이상 self.gridSize+1 미만의 랜덤값을 initialFruitColumn에 저장해준다  \n",
    "\n",
    "**initialBucketPosition = random.randrange(2, self.gridSize + 1 - 1)**  \n",
    "2이상 self.gridSize미만의 랜덤값을 initialBucketPosition에 저장해준다  \n",
    "바는 양옆에 픽셀까지 사용하기 때문에 블럭과 다르게 2이상 self.gridSize미만으로 설정해준다  \n",
    "\n",
    "**self.state = np.array([1, initialFruitColumn, initialBucketPosition])**  \n",
    "블럭의 시작은 항상 맨 위이여야 하기 때문에 self.state[0]은 1로 설정해주고 self.state[1]에는 블럭의 열번호인 initialFruitColumn로 설정해주고 self.state[2]에는 바의 위치인 initialBucketPosition을 설정해준다(바는 항상 행번호가 마지막 행번호로 정해져있기 때문에 열번호만 변경해주면 된다)\n",
    "\n",
    "**return self.getState()**  \n",
    "self.getState(현재 상태)를 리턴해준다\n",
    "\n",
    "### getState 함수  \n",
    "getState 함수는 현재 상태를 리턴해준다  \n",
    "\n",
    "**stateInfo = self.state**  \n",
    "stateInfo에 현재 state값을 불러온 뒤   \n",
    "\n",
    "**fruit_row = stateInfo[0]**  \n",
    "블럭의 행번호는 fruit_row에 저장  \n",
    "\n",
    "**fruit_col = stateInfo[1]**  \n",
    "블럭의 열번호는 fruit_col에 저장  \n",
    "\n",
    "**basket = stateInfo[2]**  \n",
    "바의 위치는 basket에 저장해준다  \n",
    "\n",
    "**return fruit_row, fruit_col, basket**  \n",
    "각각의 값들을 리턴해준다  \n",
    "\n",
    "### getReward 함수  \n",
    "getReward 함수는 보상값을 리턴해준다  \n",
    "\n",
    "**fruitRow, fruitColumn, basket = self.getState()**  \n",
    "우선 getState 함수를 이용해 현재 상태를 가져온다  \n",
    "\n",
    "**if (fruitRow == self.gridSize - 1):**   # 만약 블럭의 행번호가 self.gridSize - 1 과 같다면 (블럭이 가장 마지막 행까지 갔다면)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**if (abs(fruitColumn - basket) <= 1):**  # 블럭의 열번호 - 바의 위치에 절대값이 <= 1이라면 (0이라면 == 위치가 같다면)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**return 1**   # 1을 리턴해준다  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**else:**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**return -1**   # 위치가 같지 않다면 -1을 리턴해준다  \n",
    "**else:**   # 블럭의 행번호가 gridSize - 1 과 같지 않다면 (블럭이 아직 가장 마지막 행까지 가지 않았다면)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**return 0**   # 0을 리턴해준다**  \n",
    "\n",
    "### isGameOver 함수\n",
    "isGameOver 함수는 게임이 끝났는지의 여부를 리턴해준다   \n",
    "\n",
    "**if (self.state[0] == self.gridSize - 1):**  # 만약 블럭의 행번호가 self.girdSize-1과 같다면 (블럭이 가장 마지막 행까지 갔다면)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**return True** # True를 리턴해준다  \n",
    "**else:** # 블럭이 가장 마지막 행까지 가지 않았다면  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**return False**  # False를 리턴해준다  \n",
    "    \n",
    "### updateState 함수\n",
    "updateState 함수는 action에 따라 바의 위치를 업데이트해주고 블럭의 위치를 업데이트해준다\n",
    "\n",
    "**if (action == 1):** # action이 1이면 왼쪽으로 이동시켜준다  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**action = -1**    \n",
    "**elif (action == 2):** # action이 2이면 그대로 있는다  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**action = 0**   \n",
    "**else:**  # action이 0이면 오른쪽으로 이동시켜준다  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**action = 1**  \n",
    "\n",
    "**fruitRow, fruitColumn, basket = self.getState()**  \n",
    "getState함수를 이용해 현재상태를 가져온다  \n",
    "\n",
    "**newBasket = min(max(2, basket + action), self.gridSize - 1)**  # 바 위치 변경  \n",
    "새로운 바의 위치는 우선 2와 bascket + action 중 큰 것을 고른 다음 self.gridSize - 1과 비교하여 더 작은 값으로 설정해준다 (2이상 self.gridSize-1이하의 값으로 설정된다 바는 양옆의 픽셀까지 사용하기 때문에 2와 self.gridSize - 1로 설정해준다)  \n",
    "\n",
    "**fruitRow = fruitRow + 1**  # 블럭을 아래로 이동  \n",
    "블럭의 행번호도 하나 추가해준다  \n",
    "\n",
    "**self.state = np.array([fruitRow, fruitColumn, newBasket])**  \n",
    "state값을 새로운 값들로 업데이트 해준다  \n",
    "\n",
    "### act 함수\n",
    "act 함수는 행동을 수행한다  \n",
    "\n",
    "**self.updateState(action)**  \n",
    "updateState 함수를 사용해 바의 위치와 블럭의 위치를 업데이트 시켜준다  \n",
    "\n",
    "**reward = self.getReward()**  \n",
    "getReward 함수를 사용해 보상값을 가져오고  \n",
    "\n",
    "**gameOver = self.isGameOver()**  \n",
    "isGameOver 함수를 사용해 게임이 끝났는지 확인한다  \n",
    "\n",
    "**return self.observe(), reward, gameOver, self.getState()**  \n",
    "앞에서 가져온 값들을 리턴해준다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db22df16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CatchEnvironment():\n",
    "    # 초기화\n",
    "    def __init__(self, gridSize):\n",
    "        self.gridSize = gridSize\n",
    "        self.nbStates = self.gridSize * self.gridSize\n",
    "        self.state = np.empty(3, dtype = np.uint8) \n",
    "        \n",
    "    # 화면정보 리턴\n",
    "    def observe(self):\n",
    "        canvas = self.drawState()\n",
    "        canvas = np.reshape(canvas, (-1,self.nbStates))\n",
    "        return canvas\n",
    "    \n",
    "    # 블럭과 바를 표시하여 화면정보 리턴\n",
    "    def drawState(self):\n",
    "        canvas = np.zeros((self.gridSize, self.gridSize))\n",
    "    \n",
    "        # 블럭 표시\n",
    "        canvas[self.state[0]-1, self.state[1]-1] = 1\n",
    "\n",
    "        # 바 표시\n",
    "        canvas[self.gridSize-1, self.state[2] -1 - 1] = 1\n",
    "        canvas[self.gridSize-1, self.state[2] -1] = 1\n",
    "        canvas[self.gridSize-1, self.state[2] -1 + 1] = 1    \n",
    "        return canvas \n",
    "\n",
    "    # 블럭과 바 위치 초기화\n",
    "    def reset(self): \n",
    "        initialFruitColumn = random.randrange(1, self.gridSize + 1)\n",
    "        initialBucketPosition = random.randrange(2, self.gridSize + 1 - 1)\n",
    "        self.state = np.array([1, initialFruitColumn, initialBucketPosition]) \n",
    "        return self.getState()\n",
    "\n",
    "    # 상태 리턴\n",
    "    def getState(self):\n",
    "        stateInfo = self.state\n",
    "        fruit_row = stateInfo[0]\n",
    "        fruit_col = stateInfo[1]\n",
    "        basket = stateInfo[2]\n",
    "        return fruit_row, fruit_col, basket\n",
    "\n",
    "    # 보상값 리턴\n",
    "    def getReward(self):\n",
    "        fruitRow, fruitColumn, basket = self.getState()\n",
    "        if (fruitRow == self.gridSize - 1):  # If the fruit has reached the bottom.\n",
    "            if (abs(fruitColumn - basket) <= 1): # Check if the basket caught the fruit.\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # 게임오버 검사\n",
    "    def isGameOver(self):\n",
    "        if (self.state[0] == self.gridSize - 1): \n",
    "            return True \n",
    "        else: \n",
    "            return False \n",
    "\n",
    "    # 상태 업데이트\n",
    "    def updateState(self, action):\n",
    "        if (action == 1):\n",
    "            action = -1  # 왼쪽 이동\n",
    "        elif (action == 2):\n",
    "            action = 0  # 대기\n",
    "        else:\n",
    "            action = 1  # 오른쪽 이동\n",
    "        fruitRow, fruitColumn, basket = self.getState()\n",
    "        newBasket = min(max(2, basket + action), self.gridSize - 1)  # 바 위치 변경\n",
    "        fruitRow = fruitRow + 1  # 블럭을 아래로 이동\n",
    "        self.state = np.array([fruitRow, fruitColumn, newBasket])\n",
    "\n",
    "    # 행동 수행 (1->왼쪽, 2->대기, 3->오른쪽)\n",
    "    def act(self, action):\n",
    "        self.updateState(action)\n",
    "        reward = self.getReward()\n",
    "        gameOver = self.isGameOver()\n",
    "        return self.observe(), reward, gameOver, self.getState() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe4396c",
   "metadata": {},
   "source": [
    "## 메모리 클래스 (게임내용을 저장하고 나중에 배치로 묶어 학습에 사용)\n",
    "\n",
    "### \\_\\_init__ 함수\n",
    "\\_\\_init__ 함수는 초기화를 시켜준다  \n",
    "\n",
    "**self.maxMemory = maxMemory  \n",
    "self.gridSize = gridSize  \n",
    "self.nbStates = self.gridSize * self.gridSize  \n",
    "self.discount = discount  \n",
    "canvas = np.zeros((self.gridSize, self.gridSize))**  \n",
    "canvas는 0으로 초기화된 self.gridSize x self.gridSize의 크기를 가지는 배열이 된다  \n",
    "\n",
    "**canvas = np.reshape(canvas, (-1,self.nbStates))**  \n",
    "위에서 생성한 canvas를 self.nbStates개의 열을 가지는 배열로 재배열해준다  \n",
    "\n",
    "**self.inputState = np.empty((self.maxMemory, 100), dtype = np.float32)**  \n",
    "self.inputState는 초기화가 되지 않은 self.maxMermory * 100의 크기를 가지는 배열이 된다  \n",
    "배열의 데이터 형식은 np.float32가 된다  \n",
    "\n",
    "**self.actions = np.zeros(self.maxMemory, dtype = np.uint8)**  \n",
    "self.actions는 0으로 초기화된 self.maxMemory의 길이를 가지는 배열이 된다\n",
    "배열의 데이터 형식은 np.uint8이 된다\n",
    "\n",
    "**self.nextState = np.empty((self.maxMemory, 100), dtype = np.float32)**  \n",
    "self.nextState는 초기화가 되지 않은 self.maxMemory x 100의 크기를 가지는 배열이 된다  \n",
    "배열의 데이터 형식은 np.float32가 된다\n",
    "\n",
    "**self.gameOver = np.empty(self.maxMemory, dtype = np.bool_)**  \n",
    "self.gameOver는 초기화가 되지 않은 self.maxMemory의 길이를 가지는 배열이 된다  \n",
    "배열의 데이터 형식은 np.bool_이 된다\n",
    "\n",
    "**self.rewards = np.empty(self.maxMemory, dtype = np.int8)**  \n",
    "self.rewords는 초기화가 되지 않은 self.maxMemory의 길이를 가지는 배열이 된다  \n",
    "배열의 데이터 형식은 np.int8이 된다  \n",
    "\n",
    "**self.count = 0  \n",
    "self.current = 0**  \n",
    "\n",
    "### remember 함수\n",
    "remember 함수는 경험을 ReplayMemory에 저장해준다  \n",
    "\n",
    "**self.actions[self.current] = action**  \n",
    "self.actions의 self.current번째 데이터는 action이 된다  \n",
    "\n",
    "**self.rewards[self.current] = reward**  \n",
    "self.rewards의 self.current번째 데이터는 reward가 된다  \n",
    "\n",
    "**self.inputState[self.current, ...] = currentState**  \n",
    "self.inputState의 self.current번째 행의 데이터는 currentState가 된다  \n",
    "\n",
    "**self.nextState[self.current, ...] = nextState**   \n",
    "self.nextState의 self.current번째 행의 데이터는 nextState가 된다  \n",
    "\n",
    "**self.gameOver[self.current] = gameOver**  \n",
    "self.gameOver의 self.current번째 데이터는 gameOver가 된다  \n",
    "\n",
    "**self.count = max(self.count, self.current + 1)**  \n",
    "self.count는 self.count와 self.current+1 중 더 큰 값이 된다  \n",
    "현재까지 사용된 데이터의 수를 업데이트해준다\n",
    "\n",
    "**self.current = (self.current + 1) % self.maxMemory**  \n",
    "self.current는 self.current + 1을 self.maxMemory로 나눈 나머지값이 된다  \n",
    "이렇게 해주는 이유는 self.current + 1이 self.maxMemory 값과 같아질 경우 0으로 만들어 주기 때문이다  \n",
    "따라서 self.current는 0에서 self.maxMemory -1의 값을 가지게 된다  \n",
    "\n",
    "### getBatch 함수\n",
    "getBatch 함수는 저장된 게임내용에서 랜덤한 배치(batch)를 가져오는 함수입니다  \n",
    "배치(batch)는 한 번에 모델에 입력되는 데이터의 묶음을 말합니다  \n",
    "\n",
    "**memoryLength = self.count**  \n",
    "memoryLength는 self.count(현재까지 사용된 데이터의 수)가 된다  \n",
    "\n",
    "**chosenBatchSize = min(batchSize, memoryLength)**  \n",
    "chosenBatchSize는 batchSize와 memoryLength 중 더 작은 값이 된다  \n",
    "\n",
    "**inputs = np.zeros((chosenBatchSize, nbStates))**   \n",
    "inputs은 0으로 초기화된 chosenBatchSize x nbStates의 크기를 가지는 배열이 된다  \n",
    "\n",
    "**targets = np.zeros((chosenBatchSize, nbActions))**   \n",
    "targets은 0으로 초기화된 chosenBatchSize x nbActions의 크기를 가지는 배열이 된다  \n",
    "\n",
    "**for i in range(chosenBatchSize):** \n",
    "chosenBatchSize만큼 for문을 돌려준다  \n",
    "\n",
    "**randomIndex = random.randrange(0, memoryLength)**  \n",
    "randomIndex는 0이상 memoryLength 이하의 랜덤값을 가지게 된다  \n",
    "\n",
    "**current_inputState = np.reshape(self.inputState[randomIndex], (1, 100))**  \n",
    "current_inputState는 inputState의 randomIndex번째 행의 데이터를 1 x 100 크기의 배열로 재배열해준다  \n",
    "\n",
    "**target = sess.run(model, feed_dict={X: current_inputState})**  \n",
    "target은 tensorflow의 세션을 실행한 뒤 model을 설정해주고 X는 모델의 입력 플레이스홀더를 나타내고 current_inputState가 이 플레이스홀더에 입력이 된다  \n",
    "그 다음 모델을 실행하여 그 결과를 받을 수 있다  \n",
    "\n",
    "**current_nextState = np.reshape(self.nextState[randomIndex], (1, 100))**  \n",
    "current_nextState는 nextState의 randomIndex번째 행의 데이터를 1 x 100 크기의 배열로 재배열해준다  \n",
    "\n",
    "**current_outputs = sess.run(model, feed_dict={X: current_nextState})**       \n",
    "그 다음 target과 마찬가지로 세션을 실행한 다음 결과를 받는데 여기서는 플레이스홀더에 입력되는 값이 current_nextState로 바뀐다  \n",
    "\n",
    "**nextStateMaxQ = np.amax(current_outputs)**   \n",
    "nextStateMaxQ는 current_outputs에서 최대값을 가진다  \n",
    "가질 수 있는 가장 큰 Q값을 계산\n",
    "\n",
    "\n",
    "**if (self.gameOver[randomIndex] == True):**  # 만약 gameOver의 randomIndex번째 행의 값이 True라면 (게임이 끝났다면)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**target[0, [self.actions[randomIndex]-1]] = self.rewards[randomIndex]** # target의 첫번째 행의 self.actions의 randomIndex번째 데이터 - 1번째 열의 데이터는 self.rewards의 randomIndex번째 데이터가 된다  \n",
    "게임이 끝났을 때의 Q값은 reward(보상값)이 된다  \n",
    "\n",
    "**else:**  # 만약 gameOver의 randomIndex 행의 값이 False라면 (게임이 끝나지 않았다면)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**target[0, [self.actions[randomIndex]-1]] = self.rewards[randomIndex] +self.discount * nextStateMaxQ**  \n",
    "True일때와 마찬가지로 self.reward의 데이터를 넣어주는데 이번에는 reward(보상값)에 self.discount(감소값) x nextStateMaxQ(가질 수 있는 가장 큰 Q값)을 더해준다  \n",
    "Q값을 업데이트 시켜주는 과정인데 Q-Learning의 업데이트 수식은 보상값 + 감소값 x maxQ(상태,행동)입니다  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**inputs[i] = current_inputState**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**targets[i] = target**  \n",
    "**return inputs, targets**  \n",
    "for문이 끝나면 저장한 input과 targets을 리턴해준다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e24827a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    # 초기화\n",
    "    def __init__(self, gridSize, maxMemory, discount):\n",
    "        self.maxMemory = maxMemory\n",
    "        self.gridSize = gridSize\n",
    "        self.nbStates = self.gridSize * self.gridSize\n",
    "        self.discount = discount\n",
    "        canvas = np.zeros((self.gridSize, self.gridSize))\n",
    "        canvas = np.reshape(canvas, (-1,self.nbStates))\n",
    "        self.inputState = np.empty((self.maxMemory, 100), dtype = np.float32)\n",
    "        self.actions = np.zeros(self.maxMemory, dtype = np.uint8)\n",
    "        self.nextState = np.empty((self.maxMemory, 100), dtype = np.float32)\n",
    "        self.gameOver = np.empty(self.maxMemory, dtype = np.bool_)\n",
    "        self.rewards = np.empty(self.maxMemory, dtype = np.int8) \n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "\n",
    "    # 게임내용 추가\n",
    "    def remember(self, currentState, action, reward, nextState, gameOver):\n",
    "        self.actions[self.current] = action\n",
    "        self.rewards[self.current] = reward\n",
    "        self.inputState[self.current, ...] = currentState\n",
    "        self.nextState[self.current, ...] = nextState\n",
    "        self.gameOver[self.current] = gameOver\n",
    "        self.count = max(self.count, self.current + 1)\n",
    "        self.current = (self.current + 1) % self.maxMemory\n",
    "\n",
    "    # 게임내용을 배치로 묶어서 리턴\n",
    "    def getBatch(self, model, batchSize, nbActions, nbStates, sess, X):\n",
    "        memoryLength = self.count\n",
    "        chosenBatchSize = min(batchSize, memoryLength)\n",
    "        inputs = np.zeros((chosenBatchSize, nbStates))\n",
    "        targets = np.zeros((chosenBatchSize, nbActions))\n",
    "\n",
    "        for i in range(chosenBatchSize):\n",
    "            # 메모리에서 랜덤하게 선택\n",
    "            randomIndex = random.randrange(0, memoryLength)\n",
    "            current_inputState = np.reshape(self.inputState[randomIndex], (1, 100))\n",
    "            target = sess.run(model, feed_dict={X: current_inputState})\n",
    "\n",
    "            current_nextState = np.reshape(self.nextState[randomIndex], (1, 100))\n",
    "            current_outputs = sess.run(model, feed_dict={X: current_nextState})      \n",
    "\n",
    "            # 다음 상태의 최대 Q값\n",
    "            nextStateMaxQ = np.amax(current_outputs)\n",
    "\n",
    "\n",
    "            if (self.gameOver[randomIndex] == True):\n",
    "                # 게임오버일때 Q값은 보상값으로 설정\n",
    "                target[0, [self.actions[randomIndex]-1]] = self.rewards[randomIndex]\n",
    "            else:\n",
    "                # Q값을 계산\n",
    "                # reward + discount(gamma) * max_a' Q(s',a')\n",
    "                target[0, [self.actions[randomIndex]-1]] = self.rewards[randomIndex] +self.discount * nextStateMaxQ\n",
    "                inputs[i] = current_inputState\n",
    "                targets[i] = target\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0d14d7",
   "metadata": {},
   "source": [
    "##  메인함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5d114db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    print(\"Training new model\")\n",
    "\n",
    "    # 환경 정의\n",
    "    env = CatchEnvironment(gridSize)\n",
    "\n",
    "    # 메모리 정의\n",
    "    memory = ReplayMemory(gridSize, maxMemory, discount)\n",
    "\n",
    "    # 세이버 설정\n",
    "    saver = tf.compat.v1.train.Saver()\n",
    "    \n",
    "    winCount = 0\n",
    "    with tf.compat.v1.Session() as sess:   \n",
    "        tf.compat.v1.initialize_all_variables().run() \n",
    "\n",
    "        for i in range(epoch):\n",
    "            err = 0\n",
    "            env.reset()\n",
    "      \n",
    "            isGameOver = False\n",
    "\n",
    "            currentState = env.observe()\n",
    "            \n",
    "            while (isGameOver != True):\n",
    "                action = -9999 \n",
    "\n",
    "                # 랜덤으로 행동을 할지 Q값에 따라 행동할지 결정\n",
    "                global epsilon\n",
    "                if (randf(0, 1) <= epsilon):\n",
    "                    action = random.randrange(1, nbActions+1)\n",
    "                else:          \n",
    "                    q = sess.run(output_layer, feed_dict={X: currentState})          \n",
    "                    index = q.argmax()\n",
    "                    action = index + 1     \n",
    "\n",
    " \n",
    "\n",
    "                # 랜덤으로 행동할 확률 감소\n",
    "                if (epsilon > epsilonMinimumValue):\n",
    "                  epsilon = epsilon * 0.999\n",
    "                \n",
    "                # 행동 수행\n",
    "                nextState, reward, gameOver, stateInfo = env.act(action)\n",
    "\n",
    "                # 승리 횟수 설정\n",
    "                if (reward == 1):\n",
    "                    winCount = winCount + 1\n",
    "\n",
    "                # 메모리에 저장\n",
    "                memory.remember(currentState, action, reward, nextState, gameOver)\n",
    "\n",
    "                # 다음 상태 설정\n",
    "                currentState = nextState\n",
    "                isGameOver = gameOver\n",
    "                \n",
    "                # 입력과 출력 데이터 배치를 구함\n",
    "                inputs, targets = memory.getBatch(output_layer, batchSize, nbActions, nbStates, sess, X)\n",
    "        \n",
    "                # 학습 수행\n",
    "                _, loss = sess.run([optimizer, cost], feed_dict={X: inputs, Y: targets})  \n",
    "                err = err + loss\n",
    "\n",
    "            print(\"Epoch \" + str(i) + \": err = \" + str(err) + \": Win count = \" + str(winCount) + \" Win ratio = \" + str(float(winCount)/float(i+1)*100))\n",
    "\n",
    "        # 모델 세션 저장\n",
    "        #save_path = saver.save(sess, os.getcwd()+\"/model.ckpt\")\n",
    "        save_path = saver.save(sess,'test')\n",
    "\n",
    "        #print(\"Model saved in file: %s\" % save_path)\n",
    "        print(\"Finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a849e9c2",
   "metadata": {},
   "source": [
    "## 메인 함수 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79eeee91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new model\n",
      "WARNING:tensorflow:From C:\\Users\\Ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:288: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1108 20:35:07.650410 25552 deprecation.py:50] From C:\\Users\\Ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:288: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: err = 0.0047328555901913205: Win count = 0 Win ratio = 0.0\n",
      "Epoch 1: err = 0.0077804672182537615: Win count = 0 Win ratio = 0.0\n",
      "Epoch 2: err = 0.008060148975346237: Win count = 0 Win ratio = 0.0\n",
      "Epoch 3: err = 0.006940631661564112: Win count = 1 Win ratio = 25.0\n",
      "Epoch 4: err = 0.00773087766719982: Win count = 1 Win ratio = 20.0\n",
      "Epoch 5: err = 0.009311863803304732: Win count = 1 Win ratio = 16.666666666666664\n",
      "Epoch 6: err = 0.010141215578187257: Win count = 1 Win ratio = 14.285714285714285\n",
      "Epoch 7: err = 0.008453260525129735: Win count = 1 Win ratio = 12.5\n",
      "Epoch 8: err = 0.008302775851916522: Win count = 1 Win ratio = 11.11111111111111\n",
      "Epoch 9: err = 0.009998365887440741: Win count = 1 Win ratio = 10.0\n",
      "Epoch 10: err = 0.008982798841316253: Win count = 1 Win ratio = 9.090909090909092\n",
      "Epoch 11: err = 0.008265542157460004: Win count = 2 Win ratio = 16.666666666666664\n",
      "Epoch 12: err = 0.007598576426971704: Win count = 2 Win ratio = 15.384615384615385\n",
      "Epoch 13: err = 0.007452719670254737: Win count = 3 Win ratio = 21.428571428571427\n",
      "Epoch 14: err = 0.006321445573121309: Win count = 3 Win ratio = 20.0\n",
      "Epoch 15: err = 0.006407101405784488: Win count = 3 Win ratio = 18.75\n",
      "Epoch 16: err = 0.005979688023217022: Win count = 3 Win ratio = 17.647058823529413\n",
      "Epoch 17: err = 0.005994939478114247: Win count = 3 Win ratio = 16.666666666666664\n",
      "Epoch 18: err = 0.006010764976963401: Win count = 3 Win ratio = 15.789473684210526\n",
      "Epoch 19: err = 0.005563174898270518: Win count = 3 Win ratio = 15.0\n",
      "Epoch 20: err = 0.005724783579353243: Win count = 3 Win ratio = 14.285714285714285\n",
      "Epoch 21: err = 0.006822877941885963: Win count = 3 Win ratio = 13.636363636363635\n",
      "Epoch 22: err = 0.005536092619877309: Win count = 4 Win ratio = 17.391304347826086\n",
      "Epoch 23: err = 0.006178314972203225: Win count = 4 Win ratio = 16.666666666666664\n",
      "Epoch 24: err = 0.005443845177069306: Win count = 4 Win ratio = 16.0\n",
      "Epoch 25: err = 0.006254678766708821: Win count = 4 Win ratio = 15.384615384615385\n",
      "Epoch 26: err = 0.006016323750372976: Win count = 5 Win ratio = 18.51851851851852\n",
      "Epoch 27: err = 0.005619971023406833: Win count = 5 Win ratio = 17.857142857142858\n",
      "Epoch 28: err = 0.006195604859385639: Win count = 5 Win ratio = 17.24137931034483\n",
      "Epoch 29: err = 0.005115448555443436: Win count = 5 Win ratio = 16.666666666666664\n",
      "Epoch 30: err = 0.004737173876492307: Win count = 5 Win ratio = 16.129032258064516\n",
      "Epoch 31: err = 0.004803697025636211: Win count = 5 Win ratio = 15.625\n",
      "Epoch 32: err = 0.004799010610440746: Win count = 5 Win ratio = 15.151515151515152\n",
      "Epoch 33: err = 0.004347365291323513: Win count = 5 Win ratio = 14.705882352941178\n",
      "Epoch 34: err = 0.0050647251191549: Win count = 5 Win ratio = 14.285714285714285\n",
      "Epoch 35: err = 0.004761272080941126: Win count = 5 Win ratio = 13.88888888888889\n",
      "Epoch 36: err = 0.004426041821716353: Win count = 5 Win ratio = 13.513513513513514\n",
      "Epoch 37: err = 0.00498996049282141: Win count = 5 Win ratio = 13.157894736842104\n",
      "Epoch 38: err = 0.0038660882273688912: Win count = 5 Win ratio = 12.82051282051282\n",
      "Epoch 39: err = 0.004880681575741619: Win count = 5 Win ratio = 12.5\n",
      "Epoch 40: err = 0.004342624801211059: Win count = 6 Win ratio = 14.634146341463413\n",
      "Epoch 41: err = 0.003758050559554249: Win count = 6 Win ratio = 14.285714285714285\n",
      "Epoch 42: err = 0.004134742164751515: Win count = 7 Win ratio = 16.27906976744186\n",
      "Epoch 43: err = 0.0037070897524245083: Win count = 7 Win ratio = 15.909090909090908\n",
      "Epoch 44: err = 0.004439040931174532: Win count = 7 Win ratio = 15.555555555555555\n",
      "Epoch 45: err = 0.004094696429092437: Win count = 7 Win ratio = 15.217391304347828\n",
      "Epoch 46: err = 0.004464118450414389: Win count = 7 Win ratio = 14.893617021276595\n",
      "Epoch 47: err = 0.004299570311559364: Win count = 7 Win ratio = 14.583333333333334\n",
      "Epoch 48: err = 0.004092078976100311: Win count = 7 Win ratio = 14.285714285714285\n",
      "Epoch 49: err = 0.0035888808488380164: Win count = 8 Win ratio = 16.0\n",
      "Epoch 50: err = 0.003789458656683564: Win count = 8 Win ratio = 15.686274509803921\n",
      "Epoch 51: err = 0.0035559331881813705: Win count = 8 Win ratio = 15.384615384615385\n",
      "Epoch 52: err = 0.003321030759252608: Win count = 8 Win ratio = 15.09433962264151\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\platform\\app.py:36\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the program with an optional 'main' function and 'argv' list.\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m main \u001b[38;5;241m=\u001b[39m main \u001b[38;5;129;01mor\u001b[39;00m _sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmain\n\u001b[1;32m---> 36\u001b[0m \u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_parse_flags_tolerate_undef\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\absl\\app.py:308\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, argv, flags_parser)\u001b[0m\n\u001b[0;32m    306\u001b[0m   callback()\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 308\u001b[0m   \u001b[43m_run_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UsageError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    310\u001b[0m   usage(shorthelp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, detailed_error\u001b[38;5;241m=\u001b[39merror, exitcode\u001b[38;5;241m=\u001b[39merror\u001b[38;5;241m.\u001b[39mexitcode)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\absl\\app.py:254\u001b[0m, in \u001b[0;36m_run_main\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    252\u001b[0m   sys\u001b[38;5;241m.\u001b[39mexit(profiler\u001b[38;5;241m.\u001b[39mruncall(main, argv))\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 254\u001b[0m   sys\u001b[38;5;241m.\u001b[39mexit(\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[12], line 58\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m     55\u001b[0m isGameOver \u001b[38;5;241m=\u001b[39m gameOver\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 입력과 출력 데이터 배치를 구함\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m \u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbActions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbStates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# 학습 수행\u001b[39;00m\n\u001b[0;32m     61\u001b[0m _, loss \u001b[38;5;241m=\u001b[39m sess\u001b[38;5;241m.\u001b[39mrun([optimizer, cost], feed_dict\u001b[38;5;241m=\u001b[39m{X: inputs, Y: targets})  \n",
      "Cell \u001b[1;32mIn[11], line 43\u001b[0m, in \u001b[0;36mReplayMemory.getBatch\u001b[1;34m(self, model, batchSize, nbActions, nbStates, sess, X)\u001b[0m\n\u001b[0;32m     40\u001b[0m target \u001b[38;5;241m=\u001b[39m sess\u001b[38;5;241m.\u001b[39mrun(model, feed_dict\u001b[38;5;241m=\u001b[39m{X: current_inputState})\n\u001b[0;32m     42\u001b[0m current_nextState \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnextState[randomIndex], (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m---> 43\u001b[0m current_outputs \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mX\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_nextState\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m      \n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# 다음 상태의 최대 Q값\u001b[39;00m\n\u001b[0;32m     46\u001b[0m nextStateMaxQ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mamax(current_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:972\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    969\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 972\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    974\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    975\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1215\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1215\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1218\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1395\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1392\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1395\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1396\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1398\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1402\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1401\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1403\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1404\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1385\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1383\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1384\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1385\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1478\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1477\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1478\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1479\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.compat.v1.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e428a3",
   "metadata": {},
   "source": [
    "# 플레이 & 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac261f-2ef8-45fc-aebc-33f5c3dbd386",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41f8177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pylab as pl\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b5eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSize = 10 # The size of the grid that the agent is going to play the game on.\n",
    "maxGames = 100\n",
    "env = CatchEnvironment(gridSize)\n",
    "winCount = 0\n",
    "loseCount = 0\n",
    "numberOfGames = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52ba2090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m ground \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 2\u001b[0m plot \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m12\u001b[39m))\n\u001b[0;32m      3\u001b[0m axis \u001b[38;5;241m=\u001b[39m plot\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m111\u001b[39m, aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mequal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m axis\u001b[38;5;241m.\u001b[39mset_xlim([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m12\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "ground = 1\n",
    "plot = pl.figure(figsize=(12,12))\n",
    "axis = plot.add_subplot(111, aspect='equal')\n",
    "axis.set_xlim([-1, 12])\n",
    "axis.set_ylim([0, 12])\n",
    "\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "def drawState(fruitRow, fruitColumn, basket):\n",
    "    global gridSize\n",
    "      # column is the x axis\n",
    "    fruitX = fruitColumn \n",
    "      # Invert matrix style points to coordinates\n",
    "    fruitY = (gridSize - fruitRow + 1)\n",
    "    statusTitle = \"Wins: \" + str(winCount) + \"  Losses: \" + str(loseCount) + \"  TotalGame: \" + str(numberOfGames)\n",
    "    axis.set_title(statusTitle, fontsize=30)\n",
    "    for p in [\n",
    "        patches.Rectangle(\n",
    "            ((ground - 1), (ground)), 11, 10, facecolor=\"#000000\"      # Black\n",
    "        ),\n",
    "        patches.Rectangle(\n",
    "            (basket - 1, ground), 2, 0.5, facecolor=\"#FF0000\"     # No background\n",
    "        ),\n",
    "        patches.Rectangle(\n",
    "            (fruitX - 0.5, fruitY - 0.5), 1, 1, facecolor=\"#FF0000\"       # red \n",
    "        ),   \n",
    "        ]:\n",
    "        axis.add_patch(p)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(pl.gcf())\n",
    "\n",
    "with tf.compat.v1.Session() as sess:    \n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, 'test')\n",
    "    print('saved model is loaded!')\n",
    "\n",
    "    while (numberOfGames < maxGames):\n",
    "        numberOfGames = numberOfGames + 1\n",
    "     \n",
    "    # The initial state of the environment.\n",
    "    isGameOver = False\n",
    "    fruitRow, fruitColumn, basket = env.reset()\n",
    "    currentState = env.observe()\n",
    "    drawState(fruitRow, fruitColumn, basket)\n",
    "\n",
    "    while (isGameOver != True):\n",
    "        # Forward the current state through the network.\n",
    "        q = sess.run(output_layer, feed_dict={X: currentState})\n",
    "        # Find the max index (the chosen action).\n",
    "        index = q.argmax()\n",
    "        action = index + 1\n",
    "        nextState, reward, gameOver, stateInfo = env.act(action)    \n",
    "        fruitRow = stateInfo[0]\n",
    "        fruitColumn = stateInfo[1]\n",
    "        basket = stateInfo[2]\n",
    "     \n",
    "        # Count game results\n",
    "        if (reward == 1):\n",
    "            winCount = winCount + 1\n",
    "        elif (reward == -1):\n",
    "            loseCount = loseCount + 1\n",
    "\n",
    "        currentState = nextState\n",
    "        isGameOver = gameOver\n",
    "        drawState(fruitRow, fruitColumn, basket)\n",
    "        time.sleep(0.4)\n",
    "\n",
    "display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acd833c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222ff74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090ed7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896ad5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc68b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
