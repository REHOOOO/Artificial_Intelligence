{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb75138",
   "metadata": {},
   "source": [
    "# 강화학습 구현과 실현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f575fc67",
   "metadata": {},
   "source": [
    "강화학습에는 여러 기법이 있는데 여기에서는 DQN(Deep Q-Network)를 사용하여 강화학습을 진행하였습니다  \n",
    "DQN은 기존의 Q-Learning에 신경망을 결합한 것입니다  \n",
    "Q-Learning은 주어진 상태에서 행동을 수행하면서 미래의 효율적인 기댓값을 예측하는 Q 함수를 학습하면서 최적의 정책을 학습히는 기법입니다  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64c041",
   "metadata": {},
   "source": [
    "## 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61256157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ad440",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b5bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531a439",
   "metadata": {},
   "source": [
    "## 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb14248",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1  # 랜덤하게 행동할 확률\n",
    "epsilonMinimumValue = 0.001  # epsilon의 최소값\n",
    "nbActions = 3  # 행동의 개수 (왼쪽, 대기, 오른쪽)\n",
    "epoch = 1001  # 게임 반복횟수\n",
    "hiddenSize = 100  # 히든 레이어 뉴런 개수\n",
    "maxMemory = 500  # 게임내용을 기억하는 최대 개수\n",
    "batchSize = 50  # 학습시 데이터 묶음 개수\n",
    "gridSize = 10  # 격자 크기\n",
    "nbStates = gridSize * gridSize  # 상태 개수 (픽셀의 개수)\n",
    "discount = 0.9  # 감소값\n",
    "learningRate = 0.2  # 학습률"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3545422f",
   "metadata": {},
   "source": [
    "## 딥러닝 모델 설정\n",
    "입력레이어는 nbStates(100)개 히든레이어는 hiddenSize(100)개 출력레이어는 nbAction(3)개를 가지는 딥러닝 모델을 만들어 준다  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5d7238",
   "metadata": {},
   "source": [
    "### 입력 레이어\n",
    "X = tf.compat.v1.placeholder(tf.float32, [None, nbStates]) # 입력값  \n",
    "placeholder는 변수를 선언할 때 값을 바로 주는 것이 아닌 나중에 값을 던져줄 수 있도록 공간을 미리 만들어 주는 것이다  \n",
    "따라서 X는 데이터 유형이 float32이고 첫번째 차원의 수는 정해져있지 않고(가변적) 두번째 차원의 수는 nbStates의 값을 가지는 placeholder가 된다  \n",
    "\n",
    "W1 = tf.Variable(tf.random.truncated_normal([nbStates, hiddenSize], stddev=1.0 / math.sqrt(float(nbStates)))) # 가중치  \n",
    "입력 레이어의 가중치를 나타내는 W1을 생성해 준다  \n",
    "변수의 초기화는 랜덤값으로 주어지는데 이 랜덤값은 양쪽 끝이 잘려있는 정규분포에서 가져오게 된다\n",
    "이 정규분포의 표준편차는 $\\frac{1}{\\sqrt{nbStates}}$가 된다  \n",
    "이러한 랜덤값들로 nbStates x hiddenSize의 크기를 가지는 행렬을 채워준다  \n",
    "\n",
    "b1 = tf.Variable(tf.random.truncated_normal([hiddenSize], stddev=0.01)) # 편향  \n",
    "활성화 난이도를 조절해주는 b1을 생성해준다  \n",
    "학습데이터가 가중치와 계산되어 나온 값에 더해주어 활성화 난이도를 조절해주는 역할을 해준다  \n",
    "여기서도 양쪽 끝이 잘려있는 정규분포를 이용하게 되는데 이 정규분포의 표준편차는 0.01이 된다  \n",
    "hiddenSize개의 랜덤값들을 만들어준다  \n",
    "\n",
    "input_layer = tf.nn.relu(tf.matmul(X, W1) + b1)  \n",
    "input layer의 출력값을 만들어준다  \n",
    "우선 X과 W1의 행렬곱을 한 다음 b1을 더해준다  \n",
    "그 다음 활성화 함수에 넣어주는데 여기서 활성화 함수는 ReLU 함수가 사용된다  \n",
    "ReLU 함수: $f(x) = max(0,x)$  \n",
    "\n",
    "입력값인 X와 가중치인 W1을 행렬곱을 하게 되면 X는 None x nbStates의 크기를 가지는 행렬이고 W1은 nbStates x hiddenSize의 크기를 가지는 행렬이므로 결과는 None x hiddenSize의 크기를 가지는 행렬이 나오게 된다  \n",
    "이렇게 행렬곱을 하게되면 각각의 입력 레이어 노드에 들어가는 입력값들을 만들어낼 수 있다  \n",
    "이 값에 편향인 b1을 더해준 뒤 활성화 함수에 넣어주면 각각의 입력 레이어 노드의 출력값들을 만들어낼 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43bef925",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.compat.v1.placeholder(tf.float32, [None, nbStates]) # 입력값\n",
    "W1 = tf.Variable(tf.random.truncated_normal([nbStates, hiddenSize], stddev=1.0 / math.sqrt(float(nbStates)))) # 가중치\n",
    "b1 = tf.Variable(tf.random.truncated_normal([hiddenSize], stddev=0.01)) # 편향\n",
    "input_layer = tf.nn.relu(tf.matmul(X, W1) + b1) # 출력값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c081ec08",
   "metadata": {},
   "source": [
    "### 히든 레이어\n",
    "W2 = tf.Variable(tf.random.truncated_normal([hiddenSize, hiddenSize],stddev=1.0 / math.sqrt(float(hiddenSize))))  \n",
    "히든 레이어의 가중치를 나타내는 W2를 생성해준다\n",
    "입력 레이어와 마찬가지로 변수의 초기화는 랜덤값으로 주어지는데 이 랜덤값은 양쪽 끝이 잘려있는 정규분포에서 가져오게 된다  \n",
    "이 정규분포의 표준편차는 $\\frac{1}{\\sqrt{hiddenSize}}$가 된다  \n",
    "이러한 랜덤값들로 hiddenSize x hiddenSize의 크기를 가지는 행렬을 채워준다  \n",
    "\n",
    "b2 = tf.Variable(tf.random.truncated_normal([hiddenSize], stddev=0.01)) # 편향  \n",
    "활성화 난이도를 조절해주는 b2을 생성해준다  \n",
    "입력 레이어와 마찬가지로 학습데이터가 가중치와 계산되어 나온 값에 더해주어 활성화 난이도를 조절해주는 역할을 해준다  \n",
    "여기서도 양쪽 끝이 잘려있는 정규분포를 이용하게 되는데 이 정규분포의 표준편차는 0.01이 된다  \n",
    "hiddenSize개의 랜덤값들을 만들어준다  \n",
    "\n",
    "hidden_layer = tf.nn.relu(tf.matmul(input_layer, W2) + b2) # 출력값  \n",
    "hidden layer의 출력값을 만들어준다  \n",
    "우선 input_layer(입력 레이어의 출력값)과 W2의 행렬곱을 한 다음 b2을 더해준다  \n",
    "그 다음 활성화 함수에 넣어주는데 여기서도 활성화 함수는 ReLU 함수가 사용된다  \n",
    "ReLU 함수: $f(x) = max(0,x)$  \n",
    "\n",
    "입력 레이어의 출력값인 input_layer와 가중치인 W2을 행렬곱을 하게 되면 input_layer는 None x hiddenSize 크기를 가지는 행렬이고 W2은 hiddenSize x hiddenSize의 크기를 가지는 행렬이므로 결과는 None x hiddenSize의 크기를 가지는 행렬이 나오게 된다  \n",
    "이렇게 행렬곱을 하게되면 각각의 히든 레이어 노드에 들어가는 입력값들을 만들어낼 수 있다  \n",
    "이 값에 편향인 b2을 더해준 뒤 활성화 함수에 넣어주면 각각의 히든 레이어 노드의 출력값들을 만들어낼 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "726af6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.random.truncated_normal([hiddenSize, hiddenSize],stddev=1.0 / math.sqrt(float(hiddenSize)))) # 가중치\n",
    "b2 = tf.Variable(tf.random.truncated_normal([hiddenSize], stddev=0.01)) # 편향\n",
    "hidden_layer = tf.nn.relu(tf.matmul(input_layer, W2) + b2) # 출력값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83214b22",
   "metadata": {},
   "source": [
    "### 출력 레이어  \n",
    "W3 = tf.Variable(tf.random.truncated_normal([hiddenSize, nbActions],stddev=1.0 / math.sqrt(float(hiddenSize))))\n",
    "출력 레이어의 가중치를 나타내는 W3을 생성해준다  \n",
    "히든 레이어와 마찬가지로 변수의 초기화는 랜덤값으로 주어지는데 이 랜덤값은 양쪽 끝이 잘려있는 정규분포에서 가져오게 된다  \n",
    "이 정규분포의 표준편차는 $\\frac{1}{\\sqrt{hiddenSize}}$가 된다  \n",
    "이러한 랜덤값들로 hiddenSize x hiddenSize의 크기를 가지는 행렬을 채워준다  \n",
    "\n",
    "b3 = tf.Variable(tf.random.truncated_normal([nbActions], stddev=0.01))  \n",
    "활성화 난이도를 조절해주는 b3을 생성해준다  \n",
    "히든 레이어와 마찬가지로 학습데이터가 가중치와 계산되어 나온 값에 더해주어 활성화 난이도를 조절해주는 역할을 해준다  \n",
    "여기서도 양쪽 끝이 잘려있는 정규분포를 이용하게 되는데 이 정규분포의 표준편차는 0.01이 된다  \n",
    "bnActions개의 랜덤값들을 만들어준다   \n",
    "\n",
    "output_layer = tf.matmul(hidden_layer, W3) + b3  \n",
    "out layer의 출력값을 만들어준다\n",
    "hidden_layer(히든레이어의 출력값)과 W3의 행렬곱을 한 다음 b3을 더해준다  \n",
    "회귀 문제이기 때문에 출력 레이어에서는 활성화 함수를 사용하지 않습니다(확인 필요)(항등함수라고 함)\n",
    "\n",
    "히든 레이어의 출력값인 hidden_layer와 가중치인 W3을 행렬곱을 하게 되면 hidden_layer는 None x hiddenSize 크기를 가지는 행렬이고 W3은 hiddenSize x nbActions 크기를 가지는 행렬이므로 결과는 None x nbActions 크기를 가지는 행렬이 나오게 된다  \n",
    "이렇게 행렬곱을 하게되면 각각의 출력 레이어 노드에 들어가는 입력값들을 만들어낼 수 있다  \n",
    "이 값에 편향인 b3을 더해주면 각각의 출력 레이어 노드의 출력값들을 만들어낼 수 있다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a7fa893",
   "metadata": {},
   "outputs": [],
   "source": [
    "W3 = tf.Variable(tf.random.truncated_normal([hiddenSize, nbActions],stddev=1.0 / math.sqrt(float(hiddenSize))))\n",
    "b3 = tf.Variable(tf.random.truncated_normal([nbActions], stddev=0.01))\n",
    "output_layer = tf.matmul(hidden_layer, W3) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a38d445",
   "metadata": {},
   "source": [
    "### 목표값 플레이스홀더\n",
    "목표값 플레이스홀더 Y를 생성한다  \n",
    "Y는 데이터 유형이 float32이고 첫번째 차원의 수는 정해져있지 않고(가변적) 두번째 차원의 수는 nbActions의 값을 가지는 placeholder가 된다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faff8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.compat.v1.placeholder(tf.float32, [None, nbActions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc48f32",
   "metadata": {},
   "source": [
    "### 목표값과 출력값의 차이인 코스트\n",
    "목표값과 출력값의 오차를 구하기 위해 여기서는 평균 제곱 오차를 사용한다  \n",
    "목표값(Y)에서 출력값(output_layer)을 뺀 다음 제곱을 해준다  \n",
    "그 다음 나온 값들을 모두 더해준 뒤 2 x batchSize로 나누어준다  \n",
    "batchSize는 한 번에 모델이 학습하는 데이터 샘플의 개수로 이렇게 2 x batchSize로 나누어주게 되면 코스트를 정규화해주게 된다  \n",
    "정규화를 통해 학습률과 batchSize에 따른 코스트 크기의 영향을 줄일 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d4a5486",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_sum(tf.square(Y-output_layer)) / (2*batchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864aea12",
   "metadata": {},
   "source": [
    "### 경사하강법으로 코스트가 최소가 되는 값 찾음\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learningRate).minimize(cost)  \n",
    "경사하강법을 이용해 비용이 최소가 되는 값을 찾아준다  \n",
    "경사하강법을 사용할 때 학습률은 learningRate가 되고 minimize(cost)를 붙여주어 cost를 최소화하도록 설정해준다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c6b9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learningRate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c59ca",
   "metadata": {},
   "source": [
    "## 랜덤값 함수\n",
    "함수 randf는 s이상 e미만의 값을 가지는 랜덤값을 리턴해준다  \n",
    "(float(random.randrange(0, (e - s) * 9999)) / 10000) + s;\n",
    "random.randrange는 0부터 (e - s) * 9999 미만의 랜덤값을 만들어준다  \n",
    "이 값에 float를 붙여 실수로 변환해준 다음 10000으로 나누어주면 0이상 (e - s)미만의 랜덤값이 나오게 된다  \n",
    "여기에 s를 더해주면 s이상 e미만의 랜덤값이 된다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfdf95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randf(s, e):\n",
    "    return (float(random.randrange(0, (e - s) * 9999)) / 10000) + s;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277ba72",
   "metadata": {},
   "source": [
    "## 환경 클래스\n",
    "### \\_\\_init__ 함수\n",
    "\\_\\_init__ 함수는 초기화를 시켜준다  \n",
    "\n",
    "self.gridSize = gridSize  \n",
    "self.nbStates = self.gridSize * self.gridSize  \n",
    "self.state = np.empty(3, dtype = np.uint8)  \n",
    "np.empty는 초기화되지 않은 값으로 배열을 생성해주는 함수로 여기서는 길이가 3인 초기화되지 않은 배열이 생성된다  \n",
    "배열의 데이터 형식은 uint8(부호 없는 8비트 정수)가 된다  \n",
    "\n",
    "### observe 함수\n",
    "observe 함수는 화면정보를 리턴해준다  \n",
    "\n",
    "canvas = self.drawState()  \n",
    "drawState함수의 출력값을 canvas에 저장해준다  \n",
    "\n",
    "canvas = np.reshape(canvas, (-1,self.nbStates))  \n",
    "np.reshape는 배열을 재구성해주는 함수로 여기서는 canvas 배열을 nbStates개의 열을 가지는 배열로 재구성해준다(행의 개수는 -1로 설정되어 있으므로 자동으로 결정된다)  \n",
    "그 값을 다시 canvas에 저장해준다    \n",
    "\n",
    "return canvas  \n",
    "canvas를 리턴해준다  \n",
    "\n",
    "### drawState 함수\n",
    "drawState 함수는 블럭과 바를 표시하여 화면정보를 리턴해준다\n",
    "\n",
    "canvas = np.zeros((self.gridSize, self.gridSize))  \n",
    "모든 요소가 0으로 초기화된 self.gridSize x self.gridSize의 크기를 가지는 배열을 생성한다  \n",
    "\n",
    "canvas[self.state[0]-1, self.state[1]-1] = 1  \n",
    "self.state는 길이가 3인 배열로 0에는 블럭의 행번호가 1에는 블럭의 열번호를 저장하고 있다  \n",
    "self.state 값들을 계산할때는 gridSize를 기준으로 계산을 해 1 ~ gridSize이지만 실제로 나타낼 때는 0 ~ gridSize-1이기 때문에 각 값들에 -1을 해준 위치의 배열값을 1로 바꾸어준다   \n",
    "\n",
    "canvas[self.gridSize-1, self.state[2] -1 - 1] = 1  \n",
    "canvas[self.gridSize-1, self.state[2] -1] = 1  \n",
    "canvas[self.gridSize-1, self.state[2] -1 + 1] = 1  \n",
    "self.State[2]에는 바의 열번호를 저장하고 있다  \n",
    "따라서 바의 행번호는 gridSize-1(가장 마지막 행)을 해주고 열번호는 self.state[2]-1과 좌우 배열값들을 1로 바꾸어준다    \n",
    "\n",
    "return canvas  \n",
    "canvas를 리턴해준다  \n",
    "\n",
    "### reset 함수\n",
    "reset함수는 블럭과 바의 위치를 리셋해준다  \n",
    "\n",
    "initialFruitColumn = random.randrange(1, self.gridSize + 1)  \n",
    "1이상 gridSize+1 미만의 랜덤값을 initialFruitColumn에 저장해준다  \n",
    "\n",
    "initialBucketPosition = random.randrange(2, self.gridSize + 1 - 1)  \n",
    "2이상 gridSize미만의 랜덤값을 initialBucketPosition에 저장해준다  \n",
    "바는 양옆에 픽셀까지 사용하기 때문에 블럭과 다르게 2이상 gridSize미만으로 설정해준다  \n",
    "\n",
    "self.state = np.array([1, initialFruitColumn, initialBucketPosition])  \n",
    "블럭의 시작은 항상 맨 위이여야 하기 때문에 state[0]은 1로 설정해주고 state[1]에는 블럭의 열번호인 initialFruitColumn로 설정해주고 state[2]에는 바의 위치인 initialBucketPosition을 설정해준다(바는 항상 행번호가 마지막 행번호로 정해져있기 때문에 열번호만 변경해주면 된다)\n",
    "\n",
    "return self.getState()\n",
    "self.getState(현재 상태)를 리턴해준다\n",
    "\n",
    "### getState 함수  \n",
    "getState 함수는 현재 상태를 리턴해준다  \n",
    "\n",
    "stateInfo = self.state  \n",
    "stateInfo에 현재 state값을 불러온 뒤   \n",
    "\n",
    "fruit_row = stateInfo[0]  \n",
    "블럭의 행번호는 fruit_row에 저장  \n",
    "\n",
    "fruit_col = stateInfo[1]  \n",
    "블럭의 열번호는 fruit_col에 저장  \n",
    "\n",
    "basket = stateInfo[2]  \n",
    "바의 위치는 basket에 저장해준다  \n",
    "\n",
    "return fruit_row, fruit_col, basket  \n",
    "각각의 값들을 리턴해준다  \n",
    "\n",
    "### getReward 함수  \n",
    "getReward 함수는 보상값을 리턴해준다  \n",
    "\n",
    "fruitRow, fruitColumn, basket = self.getState()  \n",
    "우선 getState 함수를 이용해 현재 상태를 가져온다  \n",
    "\n",
    "if (fruitRow == self.gridSize - 1):   # 만약 블럭의 행번호가 gridSize - 1 과 같다면 (블럭이 가장 마지막 행까지 갔다면)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;if (abs(fruitColumn - basket) <= 1):  # 블럭의 열번호 - 바의 위치에 절대값이 <= 1이라면 (0이라면 == 위치가 같다면)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return 1   # 1을 리턴해준다  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;else:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return -1   # 위치가 같지 않다면 -1을 리턴해준다  \n",
    "else:   # 블럭의 행번호가 gridSize - 1 과 같지 않다면 (블럭이 아직 가장 마지막 행까지 가지 않았다면)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;return 0   # 0을 리턴해준다  \n",
    "\n",
    "### isGameOver 함수\n",
    "isGameOver 함수는 게임이 끝났는지의 여부를 리턴해준다   \n",
    "\n",
    "if (self.state[0] == self.gridSize - 1):  # 만약 블럭의 행번호가 girdSize-1과 같다면 (블럭이 가장 마지막 행까지 갔다면)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;return True # True를 리턴해준다  \n",
    "else: # 블럭이 가장 마지막 행까지 가지 않았다면  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;return False  # False를 리턴해준다  \n",
    "    \n",
    "### updateState 함수\n",
    "updateState 함수는 action에 따라 바의 위치를 업데이트해주고 블럭의 위치를 업데이트해준다\n",
    "\n",
    "if (action == 1): # action이 1이면 왼쪽으로 이동시켜준다  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;action = -1    \n",
    "elif (action == 2): # action이 2이면 그대로 있는다  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;action = 0   \n",
    "else:  # action이 0이면 오른쪽으로 이동시켜준다  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;action = 1  \n",
    "\n",
    "fruitRow, fruitColumn, basket = self.getState()  \n",
    "getState함수를 이용해 현재상태를 가져온다  \n",
    "\n",
    "newBasket = min(max(2, basket + action), self.gridSize - 1)  # 바 위치 변경  \n",
    "새로운 바의 위치는 우선 2와 bascket + action 중 큰 것을 고른 다음 gridSize - 1과 비교하여 더 작은 값으로 설정해준다 (2이상 gridSize-1이하의 값으로 설정된다 바는 양옆의 픽셀까지 사용하기 때문에 2와 gridSize - 1로 설정해준다)  \n",
    "\n",
    "fruitRow = fruitRow + 1  # 블럭을 아래로 이동  \n",
    "블럭의 행번호도 하나 추가해준다  \n",
    "\n",
    "self.state = np.array([fruitRow, fruitColumn, newBasket])  \n",
    "state값을 새로운 값들로 업데이트 해준다  \n",
    "\n",
    "### act 함수\n",
    "act 함수는 행동을 수행한다  \n",
    "\n",
    "self.updateState(action)  \n",
    "updateState 함수를 사용해 바의 위치와 블럭의 위치를 업데이트 시켜준다  \n",
    "\n",
    "reward = self.getReward()  \n",
    "getReward 함수를 사용해 보상값을 가져오고  \n",
    "\n",
    "gameOver = self.isGameOver()  \n",
    "isGameOver 함수를 사용해 게임이 끝났는지 확인한다  \n",
    "\n",
    "return self.observe(), reward, gameOver, self.getState()  \n",
    "앞에서 가져온 값들을 리턴해준다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db22df16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CatchEnvironment():\n",
    "    # 초기화\n",
    "    def __init__(self, gridSize):\n",
    "        self.gridSize = gridSize\n",
    "        self.nbStates = self.gridSize * self.gridSize\n",
    "        self.state = np.empty(3, dtype = np.uint8) \n",
    "        \n",
    "    # 화면정보 리턴\n",
    "    def observe(self):\n",
    "        canvas = self.drawState()\n",
    "        canvas = np.reshape(canvas, (-1,self.nbStates))\n",
    "        return canvas\n",
    "    \n",
    "    # 블럭과 바를 표시하여 화면정보 리턴\n",
    "    def drawState(self):\n",
    "        canvas = np.zeros((self.gridSize, self.gridSize))\n",
    "    \n",
    "        # 블럭 표시\n",
    "        canvas[self.state[0]-1, self.state[1]-1] = 1\n",
    "\n",
    "        # 바 표시\n",
    "        canvas[self.gridSize-1, self.state[2] -1 - 1] = 1\n",
    "        canvas[self.gridSize-1, self.state[2] -1] = 1\n",
    "        canvas[self.gridSize-1, self.state[2] -1 + 1] = 1    \n",
    "        return canvas \n",
    "\n",
    "    # 블럭과 바 위치 초기화\n",
    "    def reset(self): \n",
    "        initialFruitColumn = random.randrange(1, self.gridSize + 1)\n",
    "        initialBucketPosition = random.randrange(2, self.gridSize + 1 - 1)\n",
    "        self.state = np.array([1, initialFruitColumn, initialBucketPosition]) \n",
    "        return self.getState()\n",
    "\n",
    "    # 상태 리턴\n",
    "    def getState(self):\n",
    "        stateInfo = self.state\n",
    "        fruit_row = stateInfo[0]\n",
    "        fruit_col = stateInfo[1]\n",
    "        basket = stateInfo[2]\n",
    "        return fruit_row, fruit_col, basket\n",
    "\n",
    "    # 보상값 리턴\n",
    "    def getReward(self):\n",
    "        fruitRow, fruitColumn, basket = self.getState()\n",
    "        if (fruitRow == self.gridSize - 1):  # If the fruit has reached the bottom.\n",
    "            if (abs(fruitColumn - basket) <= 1): # Check if the basket caught the fruit.\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # 게임오버 검사\n",
    "    def isGameOver(self):\n",
    "        if (self.state[0] == self.gridSize - 1): \n",
    "            return True \n",
    "        else: \n",
    "            return False \n",
    "\n",
    "    # 상태 업데이트\n",
    "    def updateState(self, action):\n",
    "        if (action == 1):\n",
    "            action = -1  # 왼쪽 이동\n",
    "        elif (action == 2):\n",
    "            action = 0  # 대기\n",
    "        else:\n",
    "            action = 1  # 오른쪽 이동\n",
    "        fruitRow, fruitColumn, basket = self.getState()\n",
    "        newBasket = min(max(2, basket + action), self.gridSize - 1)  # 바 위치 변경\n",
    "        fruitRow = fruitRow + 1  # 블럭을 아래로 이동\n",
    "        self.state = np.array([fruitRow, fruitColumn, newBasket])\n",
    "\n",
    "    # 행동 수행 (1->왼쪽, 2->대기, 3->오른쪽)\n",
    "    def act(self, action):\n",
    "        self.updateState(action)\n",
    "        reward = self.getReward()\n",
    "        gameOver = self.isGameOver()\n",
    "        return self.observe(), reward, gameOver, self.getState() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe4396c",
   "metadata": {},
   "source": [
    "## 메모리 클래스 (게임내용을 저장하고 나중에 배치로 묶어 학습에 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e24827a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    # 초기화\n",
    "    def __init__(self, gridSize, maxMemory, discount):\n",
    "        self.maxMemory = maxMemory\n",
    "        self.gridSize = gridSize\n",
    "        self.nbStates = self.gridSize * self.gridSize\n",
    "        self.discount = discount\n",
    "        canvas = np.zeros((self.gridSize, self.gridSize))\n",
    "        canvas = np.reshape(canvas, (-1,self.nbStates))\n",
    "        self.inputState = np.empty((self.maxMemory, 100), dtype = np.float32)\n",
    "        self.actions = np.zeros(self.maxMemory, dtype = np.uint8)\n",
    "        self.nextState = np.empty((self.maxMemory, 100), dtype = np.float32)\n",
    "        self.gameOver = np.empty(self.maxMemory, dtype = np.bool_)\n",
    "        self.rewards = np.empty(self.maxMemory, dtype = np.int8) \n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "\n",
    "    # 게임내용 추가\n",
    "    def remember(self, currentState, action, reward, nextState, gameOver):\n",
    "        self.actions[self.current] = action\n",
    "        self.rewards[self.current] = reward\n",
    "        self.inputState[self.current, ...] = currentState\n",
    "        self.nextState[self.current, ...] = nextState\n",
    "        self.gameOver[self.current] = gameOver\n",
    "        self.count = max(self.count, self.current + 1)\n",
    "        self.current = (self.current + 1) % self.maxMemory\n",
    "\n",
    "    # 게임내용을 배치로 묶어서 리턴\n",
    "    def getBatch(self, model, batchSize, nbActions, nbStates, sess, X):\n",
    "        memoryLength = self.count\n",
    "        chosenBatchSize = min(batchSize, memoryLength)\n",
    "        inputs = np.zeros((chosenBatchSize, nbStates))\n",
    "        targets = np.zeros((chosenBatchSize, nbActions))\n",
    "\n",
    "        for i in range(chosenBatchSize):\n",
    "            # 메모리에서 랜덤하게 선택\n",
    "            randomIndex = random.randrange(0, memoryLength)\n",
    "            current_inputState = np.reshape(self.inputState[randomIndex], (1, 100))\n",
    "            target = sess.run(model, feed_dict={X: current_inputState})\n",
    "\n",
    "            current_nextState = np.reshape(self.nextState[randomIndex], (1, 100))\n",
    "            current_outputs = sess.run(model, feed_dict={X: current_nextState})      \n",
    "\n",
    "            # 다음 상태의 최대 Q값\n",
    "            nextStateMaxQ = np.amax(current_outputs)\n",
    "\n",
    "\n",
    "            if (self.gameOver[randomIndex] == True):\n",
    "                # 게임오버일때 Q값은 보상값으로 설정\n",
    "                target[0, [self.actions[randomIndex]-1]] = self.rewards[randomIndex]\n",
    "            else:\n",
    "                # Q값을 계산\n",
    "                # reward + discount(gamma) * max_a' Q(s',a')\n",
    "                target[0, [self.actions[randomIndex]-1]] = self.rewards[randomIndex] +self.discount * nextStateMaxQ\n",
    "                inputs[i] = current_inputState\n",
    "                targets[i] = target\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0d14d7",
   "metadata": {},
   "source": [
    "##  메인함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5d114db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    print(\"Training new model\")\n",
    "\n",
    "    # 환경 정의\n",
    "    env = CatchEnvironment(gridSize)\n",
    "\n",
    "    # 메모리 정의\n",
    "    memory = ReplayMemory(gridSize, maxMemory, discount)\n",
    "\n",
    "    # 세이버 설정\n",
    "    saver = tf.compat.v1.train.Saver()\n",
    "    \n",
    "    winCount = 0\n",
    "    with tf.compat.v1.Session() as sess:   \n",
    "        tf.compat.v1.initialize_all_variables().run() \n",
    "\n",
    "        for i in range(epoch):\n",
    "            err = 0\n",
    "            env.reset()\n",
    "      \n",
    "            isGameOver = False\n",
    "\n",
    "            currentState = env.observe()\n",
    "            \n",
    "            while (isGameOver != True):\n",
    "                action = -9999 \n",
    "\n",
    "                # 랜덤으로 행동을 할지 Q값에 따라 행동할지 결정\n",
    "                global epsilon\n",
    "                if (randf(0, 1) <= epsilon):\n",
    "                    action = random.randrange(1, nbActions+1)\n",
    "                else:          \n",
    "                    q = sess.run(output_layer, feed_dict={X: currentState})          \n",
    "                    index = q.argmax()\n",
    "                    action = index + 1     \n",
    "\n",
    " \n",
    "\n",
    "                # 랜덤으로 행동할 확률 감소\n",
    "                if (epsilon > epsilonMinimumValue):\n",
    "                  epsilon = epsilon * 0.999\n",
    "                \n",
    "                # 행동 수행\n",
    "                nextState, reward, gameOver, stateInfo = env.act(action)\n",
    "\n",
    "                # 승리 횟수 설정\n",
    "                if (reward == 1):\n",
    "                    winCount = winCount + 1\n",
    "\n",
    "                # 메모리에 저장\n",
    "                memory.remember(currentState, action, reward, nextState, gameOver)\n",
    "\n",
    "                # 다음 상태 설정\n",
    "                currentState = nextState\n",
    "                isGameOver = gameOver\n",
    "                \n",
    "                # 입력과 출력 데이터 배치를 구함\n",
    "                inputs, targets = memory.getBatch(output_layer, batchSize, nbActions, nbStates, sess, X)\n",
    "        \n",
    "                # 학습 수행\n",
    "                _, loss = sess.run([optimizer, cost], feed_dict={X: inputs, Y: targets})  \n",
    "                err = err + loss\n",
    "\n",
    "            print(\"Epoch \" + str(i) + \": err = \" + str(err) + \": Win count = \" + str(winCount) + \" Win ratio = \" + str(float(winCount)/float(i+1)*100))\n",
    "\n",
    "        # 모델 세션 저장\n",
    "        #save_path = saver.save(sess, os.getcwd()+\"/model.ckpt\")\n",
    "        save_path = saver.save(sess,'test')\n",
    "\n",
    "        #print(\"Model saved in file: %s\" % save_path)\n",
    "        print(\"Finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a849e9c2",
   "metadata": {},
   "source": [
    "## 메인 함수 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79eeee91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new model\n",
      "WARNING:tensorflow:From C:\\Users\\Ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:288: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1108 20:35:07.650410 25552 deprecation.py:50] From C:\\Users\\Ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:288: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: err = 0.0047328555901913205: Win count = 0 Win ratio = 0.0\n",
      "Epoch 1: err = 0.0077804672182537615: Win count = 0 Win ratio = 0.0\n",
      "Epoch 2: err = 0.008060148975346237: Win count = 0 Win ratio = 0.0\n",
      "Epoch 3: err = 0.006940631661564112: Win count = 1 Win ratio = 25.0\n",
      "Epoch 4: err = 0.00773087766719982: Win count = 1 Win ratio = 20.0\n",
      "Epoch 5: err = 0.009311863803304732: Win count = 1 Win ratio = 16.666666666666664\n",
      "Epoch 6: err = 0.010141215578187257: Win count = 1 Win ratio = 14.285714285714285\n",
      "Epoch 7: err = 0.008453260525129735: Win count = 1 Win ratio = 12.5\n",
      "Epoch 8: err = 0.008302775851916522: Win count = 1 Win ratio = 11.11111111111111\n",
      "Epoch 9: err = 0.009998365887440741: Win count = 1 Win ratio = 10.0\n",
      "Epoch 10: err = 0.008982798841316253: Win count = 1 Win ratio = 9.090909090909092\n",
      "Epoch 11: err = 0.008265542157460004: Win count = 2 Win ratio = 16.666666666666664\n",
      "Epoch 12: err = 0.007598576426971704: Win count = 2 Win ratio = 15.384615384615385\n",
      "Epoch 13: err = 0.007452719670254737: Win count = 3 Win ratio = 21.428571428571427\n",
      "Epoch 14: err = 0.006321445573121309: Win count = 3 Win ratio = 20.0\n",
      "Epoch 15: err = 0.006407101405784488: Win count = 3 Win ratio = 18.75\n",
      "Epoch 16: err = 0.005979688023217022: Win count = 3 Win ratio = 17.647058823529413\n",
      "Epoch 17: err = 0.005994939478114247: Win count = 3 Win ratio = 16.666666666666664\n",
      "Epoch 18: err = 0.006010764976963401: Win count = 3 Win ratio = 15.789473684210526\n",
      "Epoch 19: err = 0.005563174898270518: Win count = 3 Win ratio = 15.0\n",
      "Epoch 20: err = 0.005724783579353243: Win count = 3 Win ratio = 14.285714285714285\n",
      "Epoch 21: err = 0.006822877941885963: Win count = 3 Win ratio = 13.636363636363635\n",
      "Epoch 22: err = 0.005536092619877309: Win count = 4 Win ratio = 17.391304347826086\n",
      "Epoch 23: err = 0.006178314972203225: Win count = 4 Win ratio = 16.666666666666664\n",
      "Epoch 24: err = 0.005443845177069306: Win count = 4 Win ratio = 16.0\n",
      "Epoch 25: err = 0.006254678766708821: Win count = 4 Win ratio = 15.384615384615385\n",
      "Epoch 26: err = 0.006016323750372976: Win count = 5 Win ratio = 18.51851851851852\n",
      "Epoch 27: err = 0.005619971023406833: Win count = 5 Win ratio = 17.857142857142858\n",
      "Epoch 28: err = 0.006195604859385639: Win count = 5 Win ratio = 17.24137931034483\n",
      "Epoch 29: err = 0.005115448555443436: Win count = 5 Win ratio = 16.666666666666664\n",
      "Epoch 30: err = 0.004737173876492307: Win count = 5 Win ratio = 16.129032258064516\n",
      "Epoch 31: err = 0.004803697025636211: Win count = 5 Win ratio = 15.625\n",
      "Epoch 32: err = 0.004799010610440746: Win count = 5 Win ratio = 15.151515151515152\n",
      "Epoch 33: err = 0.004347365291323513: Win count = 5 Win ratio = 14.705882352941178\n",
      "Epoch 34: err = 0.0050647251191549: Win count = 5 Win ratio = 14.285714285714285\n",
      "Epoch 35: err = 0.004761272080941126: Win count = 5 Win ratio = 13.88888888888889\n",
      "Epoch 36: err = 0.004426041821716353: Win count = 5 Win ratio = 13.513513513513514\n",
      "Epoch 37: err = 0.00498996049282141: Win count = 5 Win ratio = 13.157894736842104\n",
      "Epoch 38: err = 0.0038660882273688912: Win count = 5 Win ratio = 12.82051282051282\n",
      "Epoch 39: err = 0.004880681575741619: Win count = 5 Win ratio = 12.5\n",
      "Epoch 40: err = 0.004342624801211059: Win count = 6 Win ratio = 14.634146341463413\n",
      "Epoch 41: err = 0.003758050559554249: Win count = 6 Win ratio = 14.285714285714285\n",
      "Epoch 42: err = 0.004134742164751515: Win count = 7 Win ratio = 16.27906976744186\n",
      "Epoch 43: err = 0.0037070897524245083: Win count = 7 Win ratio = 15.909090909090908\n",
      "Epoch 44: err = 0.004439040931174532: Win count = 7 Win ratio = 15.555555555555555\n",
      "Epoch 45: err = 0.004094696429092437: Win count = 7 Win ratio = 15.217391304347828\n",
      "Epoch 46: err = 0.004464118450414389: Win count = 7 Win ratio = 14.893617021276595\n",
      "Epoch 47: err = 0.004299570311559364: Win count = 7 Win ratio = 14.583333333333334\n",
      "Epoch 48: err = 0.004092078976100311: Win count = 7 Win ratio = 14.285714285714285\n",
      "Epoch 49: err = 0.0035888808488380164: Win count = 8 Win ratio = 16.0\n",
      "Epoch 50: err = 0.003789458656683564: Win count = 8 Win ratio = 15.686274509803921\n",
      "Epoch 51: err = 0.0035559331881813705: Win count = 8 Win ratio = 15.384615384615385\n",
      "Epoch 52: err = 0.003321030759252608: Win count = 8 Win ratio = 15.09433962264151\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\platform\\app.py:36\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the program with an optional 'main' function and 'argv' list.\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m main \u001b[38;5;241m=\u001b[39m main \u001b[38;5;129;01mor\u001b[39;00m _sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmain\n\u001b[1;32m---> 36\u001b[0m \u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_parse_flags_tolerate_undef\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\absl\\app.py:308\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, argv, flags_parser)\u001b[0m\n\u001b[0;32m    306\u001b[0m   callback()\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 308\u001b[0m   \u001b[43m_run_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UsageError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    310\u001b[0m   usage(shorthelp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, detailed_error\u001b[38;5;241m=\u001b[39merror, exitcode\u001b[38;5;241m=\u001b[39merror\u001b[38;5;241m.\u001b[39mexitcode)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\absl\\app.py:254\u001b[0m, in \u001b[0;36m_run_main\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    252\u001b[0m   sys\u001b[38;5;241m.\u001b[39mexit(profiler\u001b[38;5;241m.\u001b[39mruncall(main, argv))\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 254\u001b[0m   sys\u001b[38;5;241m.\u001b[39mexit(\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[12], line 58\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m     55\u001b[0m isGameOver \u001b[38;5;241m=\u001b[39m gameOver\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 입력과 출력 데이터 배치를 구함\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m \u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbActions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbStates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# 학습 수행\u001b[39;00m\n\u001b[0;32m     61\u001b[0m _, loss \u001b[38;5;241m=\u001b[39m sess\u001b[38;5;241m.\u001b[39mrun([optimizer, cost], feed_dict\u001b[38;5;241m=\u001b[39m{X: inputs, Y: targets})  \n",
      "Cell \u001b[1;32mIn[11], line 43\u001b[0m, in \u001b[0;36mReplayMemory.getBatch\u001b[1;34m(self, model, batchSize, nbActions, nbStates, sess, X)\u001b[0m\n\u001b[0;32m     40\u001b[0m target \u001b[38;5;241m=\u001b[39m sess\u001b[38;5;241m.\u001b[39mrun(model, feed_dict\u001b[38;5;241m=\u001b[39m{X: current_inputState})\n\u001b[0;32m     42\u001b[0m current_nextState \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnextState[randomIndex], (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m---> 43\u001b[0m current_outputs \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mX\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_nextState\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m      \n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# 다음 상태의 최대 Q값\u001b[39;00m\n\u001b[0;32m     46\u001b[0m nextStateMaxQ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mamax(current_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:972\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    969\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 972\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    974\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    975\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1215\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1215\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1218\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1395\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1392\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1395\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1396\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1398\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1402\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1401\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1403\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1404\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1385\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1383\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1384\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1385\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1478\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1477\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1478\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1479\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.compat.v1.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e428a3",
   "metadata": {},
   "source": [
    "# 플레이 & 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac261f-2ef8-45fc-aebc-33f5c3dbd386",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41f8177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pylab as pl\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b5eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSize = 10 # The size of the grid that the agent is going to play the game on.\n",
    "maxGames = 100\n",
    "env = CatchEnvironment(gridSize)\n",
    "winCount = 0\n",
    "loseCount = 0\n",
    "numberOfGames = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52ba2090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ground = 1\n",
    "plot = pl.figure(figsize=(12,12))\n",
    "axis = plot.add_subplot(111, aspect='equal')\n",
    "axis.set_xlim([-1, 12])\n",
    "axis.set_ylim([0, 12])\n",
    "\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "def drawState(fruitRow, fruitColumn, basket):\n",
    "    global gridSize\n",
    "      # column is the x axis\n",
    "    fruitX = fruitColumn \n",
    "      # Invert matrix style points to coordinates\n",
    "    fruitY = (gridSize - fruitRow + 1)\n",
    "    statusTitle = \"Wins: \" + str(winCount) + \"  Losses: \" + str(loseCount) + \"  TotalGame: \" + str(numberOfGames)\n",
    "    axis.set_title(statusTitle, fontsize=30)\n",
    "    for p in [\n",
    "        patches.Rectangle(\n",
    "            ((ground - 1), (ground)), 11, 10, facecolor=\"#000000\"      # Black\n",
    "        ),\n",
    "        patches.Rectangle(\n",
    "            (basket - 1, ground), 2, 0.5, facecolor=\"#FF0000\"     # No background\n",
    "        ),\n",
    "        patches.Rectangle(\n",
    "            (fruitX - 0.5, fruitY - 0.5), 1, 1, facecolor=\"#FF0000\"       # red \n",
    "        ),   \n",
    "        ]:\n",
    "        axis.add_patch(p)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(pl.gcf())\n",
    "\n",
    "with tf.compat.v1.Session() as sess:    \n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, 'test')\n",
    "    print('saved model is loaded!')\n",
    "\n",
    "    while (numberOfGames < maxGames):\n",
    "        numberOfGames = numberOfGames + 1\n",
    "     \n",
    "    # The initial state of the environment.\n",
    "    isGameOver = False\n",
    "    fruitRow, fruitColumn, basket = env.reset()\n",
    "    currentState = env.observe()\n",
    "    drawState(fruitRow, fruitColumn, basket)\n",
    "\n",
    "    while (isGameOver != True):\n",
    "        # Forward the current state through the network.\n",
    "        q = sess.run(output_layer, feed_dict={X: currentState})\n",
    "        # Find the max index (the chosen action).\n",
    "        index = q.argmax()\n",
    "        action = index + 1\n",
    "        nextState, reward, gameOver, stateInfo = env.act(action)    \n",
    "        fruitRow = stateInfo[0]\n",
    "        fruitColumn = stateInfo[1]\n",
    "        basket = stateInfo[2]\n",
    "     \n",
    "        # Count game results\n",
    "        if (reward == 1):\n",
    "            winCount = winCount + 1\n",
    "        elif (reward == -1):\n",
    "            loseCount = loseCount + 1\n",
    "\n",
    "        currentState = nextState\n",
    "        isGameOver = gameOver\n",
    "        drawState(fruitRow, fruitColumn, basket)\n",
    "        time.sleep(0.4)\n",
    "\n",
    "display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd833c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222ff74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090ed7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896ad5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc68b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
