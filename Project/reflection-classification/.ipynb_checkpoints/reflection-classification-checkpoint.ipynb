{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9acf28c",
   "metadata": {},
   "source": [
    "# install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "602dc7ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 123.5/123.5 kB 7.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.10.3-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp310-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.0-cp310-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.1/7.9 MB 34.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.5/7.9 MB 32.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.3/7.9 MB 39.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.7/7.9 MB 40.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.9/7.9 MB 40.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 38.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 36.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 311.7/311.7 kB ? eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp310-cp310-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 269.6/269.6 kB ? eta 0:00:00\n",
      "Downloading safetensors-0.4.0-cp310-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 277.4/277.4 kB 16.7 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 1.8/2.2 MB 58.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 46.6 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 4.3 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.19.4 regex-2023.10.3 safetensors-0.4.0 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-win_amd64.whl (977 kB)\n",
      "     ---------------------------------------- 0.0/977.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 977.5/977.5 kB 31.2 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "Requirement already satisfied: protobuf in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.24.4)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp310-cp310-win_amd64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.11.4)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading gensim-4.3.2-cp310-cp310-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.7/24.0 MB 53.8 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 3.5/24.0 MB 44.6 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 5.3/24.0 MB 48.6 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 6.6/24.0 MB 46.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 8.1/24.0 MB 43.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 9.4/24.0 MB 42.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 10.7/24.0 MB 43.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.1/24.0 MB 43.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 13.5/24.0 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.9/24.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 16.3/24.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 17.6/24.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.2/24.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 20.6/24.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.0/24.0 MB 40.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.3/24.0 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 34.4 MB/s eta 0:00:00\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.0/57.0 kB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-6.4.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install protobuf\n",
    "!pip install gensim\n",
    "!pip install -U scikit-learn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362503eb",
   "metadata": {},
   "source": [
    "# data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9ddf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import dataclasses\n",
    "from typing import Iterable, Optional, Union, Dict, Tuple, List\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Sentence:\n",
    "    id: int\n",
    "    text: str\n",
    "    context: str\n",
    "    label: str\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Union[str, int]]:\n",
    "        return dataclasses.asdict(self)\n",
    "\n",
    "\n",
    "def balance_targets(sentences: Iterable[Sentence], method: str = \"downsample_o_cat\", shuffle=True) \\\n",
    "        -> Iterable[Sentence]:\n",
    "    \"\"\"\n",
    "    Oversamples and/or undersamples training sentences by a number of targets.\n",
    "    This is useful for linear shallow classifiers, that are prone to simply overfit the most-occurring category.\n",
    "    See the source code for a documentation of resample methods logic\n",
    "    :param shuffle: whether to shuffle the output\n",
    "    :param sentences: sentences to resample\n",
    "    :param method: resample method, one of {downsample_o_cat, downsample_o_pzk_cats, all_upsampled, remove_o_cat}\n",
    "    :return: resampled, possibly shuffled input sentences\n",
    "    \"\"\"\n",
    "    import random\n",
    "    # take the second-top count from categories apart from \"Other\"\n",
    "    targets = [s.label for s in sentences]\n",
    "    second_top_count = sorted([sum([target == cat for target in targets]) for cat in set(targets) - {\"O\"}])[-2]\n",
    "    if method == \"downsample_o_cat\":\n",
    "        # downsample \"other\" category to second-most-occurring category count\n",
    "        out_sentences = list((random.sample([s for s in sentences if s.label == \"O\"], second_top_count) +\n",
    "                         [s for s in sentences if s.label != \"O\"]))\n",
    "    elif method == \"downsample_o_pzk_cats\":\n",
    "        # downsample \"other\" + \"P_ZK\" (experience description) category to third-most-occurring category count\n",
    "        out_sentences = list((random.sample([s for s in sentences if s.label == \"O\"], second_top_count) +\n",
    "                         [s for s in sentences if s.label != \"O\"]))\n",
    "        out_sentences = list((random.sample([s for s in out_sentences if s.label == \"P_ZK\"], second_top_count) +\n",
    "                         [s for s in out_sentences if s.label != \"P_ZK\"]))\n",
    "    elif method == \"all_upsampled\":\n",
    "        # upsample all categories to a count of most-occurring one (presumably \"other\" category)\n",
    "        from itertools import chain\n",
    "        out_sentences = list(itertools.chain(*[random.choices([s for s in sentences if s.label == cat],\n",
    "                                                              k=second_top_count) for cat in set(targets)]))\n",
    "    elif method == \"remove_o_cat\":\n",
    "        # completely remove sentences of \"other\" category\n",
    "        out_sentences = [s for s in sentences if s.label != \"O\"]\n",
    "    else:\n",
    "        out_sentences = sentences\n",
    "    if shuffle:\n",
    "        # random shuffle output sentences\n",
    "        random.shuffle(out_sentences)\n",
    "    return out_sentences\n",
    "\n",
    "\n",
    "def get_sentence_vertical(sentences_dir: str, confidence_thrd: Optional[int] = 0) -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Creates a tab-separated csv table with sentences_text, tags, users and sources, in out_table_path\n",
    "    :param sentences_dir: directory of input sentences, divided to [train, val, test] subdirectories\n",
    "    :param confidence_thrd: minimal mean confidence threshold of the retrieved sentences\n",
    "    :return: Dataframe with attributes of retrieved sentences\n",
    "    \"\"\"\n",
    "    from itertools import chain\n",
    "    import pandas as pd  # if you need this, run 'pip install pandas==1.2.1'\n",
    "    from utils.dataset import ReflexiveDataset\n",
    "\n",
    "    sentences_splits = [ReflexiveDataset.sentences_from_tsv(sentences_dir, dataset_type, confidence_thrd,\n",
    "                                                            use_context=True)\n",
    "                        for dataset_type in [\"train\", \"val\", \"test\"]]\n",
    "    out_vertical = pd.DataFrame.from_records([s.to_dict() for s in chain(*sentences_splits)])\n",
    "    return out_vertical\n",
    "\n",
    "\n",
    "def split_text_to_sentence_context(text: str, sep_chars: Tuple[str] = (\".\", \"?\", \"!\")) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Splits the input text to sentences with the corresponding context,\n",
    "    in the format compliant with the training of NeuralClassifier\n",
    "    :param text: Full input paragraph, e.g. whole reflective diary, to extract the sentences to classify\n",
    "    :param sep_chars: characters separating potential sentences\n",
    "    \"\"\"\n",
    "    out_sentences = []\n",
    "    current_sent = []\n",
    "    words = text.split()\n",
    "\n",
    "    for w_i, word in enumerate(words):\n",
    "        current_sent.append(word)\n",
    "        is_last_or_is_upper = (w_i == len(words)-1 or words[w_i+1][0].isupper())\n",
    "        if any(word.endswith(mark) for mark in sep_chars) and is_last_or_is_upper:\n",
    "            out_sentences.append(\" \".join(current_sent))\n",
    "            current_sent = []\n",
    "\n",
    "    for sent_i, sent in enumerate(out_sentences):\n",
    "        context = \" \".join(out_sentences[sent_i-2:sent_i+2])\n",
    "        yield sent, context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7e10ff",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d2c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import List, Union, Dict\n",
    "import ast\n",
    "\n",
    "from filelock import FileLock\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer, InputFeatures, logging\n",
    "import pandas as pd\n",
    "# from .data_reader import Sentence\n",
    "\n",
    "logger = logging.get_logger()\n",
    "\n",
    "# original labels:\n",
    "# LABELS = [\"O\", \"OS_PRES\", \"PERS\", \"POC\", \"P_ZK\", \"REF_P\", \"UV_OBT\", \"VY_IN\", \"VY_VY\"]\n",
    "\n",
    "LABELS = [\"Other\", \"Belief\", \"Perspective\", \"Feeling\", \"Experience\",\n",
    "          \"Reflection\", \"Difficulty\", \"Intention\", \"Learning\"]\n",
    "\n",
    "\n",
    "class Split(Enum):\n",
    "    train = \"train\"\n",
    "    eval = \"eval\"\n",
    "    test = \"test\"\n",
    "\n",
    "\n",
    "class ReflexiveDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, sentences_dir: str, dataset_type: str, cache_dir: str, label_list: List[str],\n",
    "                 tokenizer: Union[AutoTokenizer, PreTrainedTokenizer],\n",
    "                 use_context=True, mean_confidence_threshold: int = 5):\n",
    "        self.sentences_dir = sentences_dir\n",
    "        self.confidence_thrd = mean_confidence_threshold\n",
    "        self.dataset_type = dataset_type\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_list = label_list\n",
    "        self.label_map = {label: i for i, label in enumerate(label_list)}\n",
    "        self.use_context = use_context\n",
    "\n",
    "        if not Path(cache_dir).exists():\n",
    "            Path(cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        cached_features_file = os.path.join(\n",
    "            cache_dir,\n",
    "            \"cached_{}_{}_{}\".format(dataset_type, tokenizer.__class__.__name__, str(self.tokenizer.model_max_length)),\n",
    "        )\n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "            logger.info(f\"Creating features from reflexive diaries\")\n",
    "            self.sentences = self.sentences_from_tsv(sentences_dir, dataset_type, self.confidence_thrd, self.use_context)\n",
    "            self.features = self.convert_examples_to_features(self.sentences)\n",
    "\n",
    "    @staticmethod\n",
    "    def sentences_from_tsv(sentences_dir: str, dataset_type: str,\n",
    "                           confidence_thrd: int, use_context: bool) -> List[Sentence]:\n",
    "        \"\"\"Creates sentences for the training, eval and test sets.\"\"\"\n",
    "        tsv_path = os.path.join(sentences_dir, dataset_type, \"sentences.tsv\")\n",
    "        df = pd.read_csv(tsv_path, sep='\\t')\n",
    "        df.sentence = df.sentence.fillna(\"\")\n",
    "        df.context = df.context.fillna(\"\")\n",
    "        sentences = []\n",
    "        # group by sources, iterate every group separately, to avoid context overlays\n",
    "        for idx, row in enumerate(df.itertuples()):\n",
    "            confidences = ast.literal_eval(row.confidence)\n",
    "            if sum(confidences) / len(confidences) >= confidence_thrd:\n",
    "                sentences.append(Sentence(id=row.idx, text=row.sentence,\n",
    "                                          context=row.context if use_context else None,\n",
    "                                          label=row.y))\n",
    "        logger.info(\"Retrieving %s of all %s %s sentences, over threshold %s\" %\n",
    "                    (len(sentences), len(df), dataset_type, confidence_thrd))\n",
    "        return sentences\n",
    "    \n",
    "    def convert_examples_to_features(self, examples: List[Sentence]) -> List[Dict[str, List[int]]]:\n",
    "\n",
    "        batch_encoding = self.tokenizer(\n",
    "            text=[example.text.strip() for example in examples],\n",
    "            text_pair=[e.context.strip() for e in examples] if self.use_context else None,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        features = []\n",
    "        for i in range(len(examples)):\n",
    "            inputs = {k: batch_encoding[k][i] for k in batch_encoding if k != \"token_type_ids\"}\n",
    "            inputs[\"label\"] = self.label_map[examples[i].label]\n",
    "            features.append(inputs)\n",
    "\n",
    "        for i, example in enumerate(examples[:5]):\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"id: %s\" % (example.id))\n",
    "            logger.info(\"features: %s\" % features[i])\n",
    "\n",
    "        return features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i) -> InputFeatures:\n",
    "        return self.features[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eac5743",
   "metadata": {},
   "source": [
    "# shallow_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bccbde54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from gensim import corpora\n",
    "from gensim import matutils\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#from .utils.dataset import ReflexiveDataset\n",
    "#from .utils.data_reader import Sentence\n",
    "\n",
    "\n",
    "class ShallowClassifier:\n",
    "    word_dictionary = None\n",
    "\n",
    "    def __init__(self, classifier, use_context: bool, bow_size: int,\n",
    "                 sentences_dir: Optional[str] = None, lang: str = \"cze\"):\n",
    "        self.classifier = classifier\n",
    "        self.sentences_dir = sentences_dir\n",
    "        self.use_context = use_context\n",
    "        self.bow_size = bow_size\n",
    "        self.lang = lang\n",
    "\n",
    "    def _preprocess_string(self, text: str) -> List[str]:\n",
    "        from gensim.parsing import preprocess_string\n",
    "        if self.lang == \"cze\":\n",
    "            from utils.cs_stemmer import cz_stem\n",
    "            return [cz_stem(word) for word in preprocess_string(text)]\n",
    "        else:\n",
    "            return preprocess_string(text)\n",
    "\n",
    "    def _initialize_bow_model(self, sents: List[Sentence]):\n",
    "        text_preprocessed = [self._preprocess_string(str(s.text)) for s in sents]\n",
    "        contexts_preprocessed = [self._preprocess_string(str(s.context)) for s in sents]\n",
    "\n",
    "        self.word_dictionary = corpora.Dictionary(text_preprocessed + contexts_preprocessed)\n",
    "        # keep most-occurring 10k words\n",
    "        # we need to check this with Ullmann\n",
    "        self.word_dictionary.filter_extremes(keep_n=self.bow_size)\n",
    "\n",
    "    def _vectorize_sentences(self, sents: List[Sentence]):\n",
    "        text_preprocessed = [self._preprocess_string(s.text) for s in sents]\n",
    "        # sparse matrix contains just pairs of co-occurrences\n",
    "        sparse_matrix = [self.word_dictionary.doc2bow(t) for t in text_preprocessed]\n",
    "        # we want to get natural, dense vectors for each document, containing the most-frequent num_terms\n",
    "        dense_matrix = matutils.corpus2dense(sparse_matrix, num_terms=self.bow_size).transpose()\n",
    "        if not self.use_context:\n",
    "            return dense_matrix\n",
    "        else:\n",
    "            # the same for contextual vectors\n",
    "            text_preprocessed_c = [self._preprocess_string(str(s.context)) for s in sents]\n",
    "            # sparse matrix contains just pairs of co-occurrences\n",
    "            sparse_matrix_c = [self.word_dictionary.doc2bow(t) for t in text_preprocessed_c]\n",
    "            # we want to get natural, dense vectors for each document, containing the most-frequent num_terms\n",
    "            dense_matrix_c = matutils.corpus2dense(sparse_matrix_c, num_terms=self.bow_size).transpose()\n",
    "\n",
    "            # concat textual and contextual vectors horizontally\n",
    "            return np.hstack([dense_matrix, dense_matrix_c])\n",
    "\n",
    "    def train(self, in_sentences: List[Sentence] = None, confidence_thrd: int = 5):\n",
    "        if in_sentences is None:\n",
    "            # get the dataset from outside\n",
    "            sentences = ReflexiveDataset.sentences_from_tsv(self.sentences_dir, \"train\",\n",
    "                                                            confidence_thrd, self.use_context)\n",
    "        else:\n",
    "            # user gets the dataset himself\n",
    "            sentences = in_sentences\n",
    "\n",
    "        self._initialize_bow_model(sentences)\n",
    "        vectors = self._vectorize_sentences(sentences)\n",
    "        self.classifier.fit(vectors, [s.label for s in in_sentences])\n",
    "\n",
    "    def predict(self, sentences: List[Sentence]):\n",
    "        vectors = self._vectorize_sentences(sentences)\n",
    "        targets = self.classifier.predict(vectors)\n",
    "        return targets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f5f770",
   "metadata": {},
   "source": [
    "# train_eval_shallow_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "227d3281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reflection-classification/data/sentences/en\\\\train\\\\sentences.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized classifier: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m classifier)\n\u001b[1;32m---> 35\u001b[0m train_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mReflexiveDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_tsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mtrain_confidence_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m test_sentences \u001b[38;5;241m=\u001b[39m ReflexiveDataset\u001b[38;5;241m.\u001b[39msentences_from_tsv(sentences_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m                                                      test_confidence_threshold, use_context)\n\u001b[0;32m     40\u001b[0m cfr \u001b[38;5;241m=\u001b[39m ShallowClassifier(classifier\u001b[38;5;241m=\u001b[39mclassifier, use_context\u001b[38;5;241m=\u001b[39muse_context, bow_size\u001b[38;5;241m=\u001b[39mvocabulary_size,\n\u001b[0;32m     41\u001b[0m                         lang\u001b[38;5;241m=\u001b[39mlanguage)\n",
      "Cell \u001b[1;32mIn[2], line 61\u001b[0m, in \u001b[0;36mReflexiveDataset.sentences_from_tsv\u001b[1;34m(sentences_dir, dataset_type, confidence_thrd, use_context)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates sentences for the training, eval and test sets.\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m tsv_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sentences_dir, dataset_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentences.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m df\u001b[38;5;241m.\u001b[39msentence \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msentence\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m df\u001b[38;5;241m.\u001b[39mcontext \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reflection-classification/data/sentences/en\\\\train\\\\sentences.tsv'"
     ]
    }
   ],
   "source": [
    "#from reflection_classification.utils.dataset import ReflexiveDataset\n",
    "#from reflection_classification.shallow_classifier import ShallowClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "#import argparse\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 하이퍼 파라미터 \n",
    "    classifier = 'random_forrest' # 분류기 선택 \n",
    "    sentences_dir = 'reflection-classification/data/sentences/en' # 데이터 위치 \n",
    "    train_confidence_threshold = 5\n",
    "    test_confidence_threshold = 5\n",
    "    use_context = True\n",
    "    vocabulary_size = 800\n",
    "    language ='en'\n",
    "    \n",
    "    # 분류기 설정 \n",
    "    if classifier == 'random_forrest':\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        classifier = RandomForestClassifier()\n",
    "    elif classifier == 'logistic_regression':\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        classifier = LogisticRegression(max_iter=10e4)\n",
    "    elif classifier == 'naive_bayes':\n",
    "        from sklearn.naive_bayes import MultinomialNB\n",
    "        classifier = MultinomialNB()\n",
    "    elif classifier == 'support_vector_classifier':\n",
    "        from sklearn.svm import SVC\n",
    "        classifier = SVC()\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized classifier: %s\" % classifier)\n",
    "\n",
    "    train_sentences = ReflexiveDataset.sentences_from_tsv(sentences_dir, \"train\",\n",
    "                                                          train_confidence_threshold, use_context)\n",
    "    test_sentences = ReflexiveDataset.sentences_from_tsv(sentences_dir, \"test\",\n",
    "                                                         test_confidence_threshold, use_context)\n",
    "\n",
    "    cfr = ShallowClassifier(classifier=classifier, use_context=use_context, bow_size=vocabulary_size,\n",
    "                            lang=language)\n",
    "    cfr.train(train_sentences)\n",
    "    pred_targets = cfr.predict(test_sentences)\n",
    "    true_targets = [s.label for s in test_sentences]\n",
    "    objective_val = f1_score(true_targets, pred_targets, average='micro')\n",
    "    print(\"Evaluating on %s sentences\" % len(test_sentences))\n",
    "    print(\"Classification report: \\n%s\" % classification_report(true_targets, pred_targets))\n",
    "    print(objective_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f761abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73759b34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
