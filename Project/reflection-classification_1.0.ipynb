{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9acf28c",
   "metadata": {
    "id": "a9acf28c"
   },
   "source": [
    "# install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "602dc7ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68008,
     "status": "ok",
     "timestamp": 1700908189250,
     "user": {
      "displayName": "스파르탄",
      "userId": "04034795070506982642"
     },
     "user_tz": -540
    },
    "id": "602dc7ef",
    "outputId": "70519a67-4ce6-49ac-e671-da9b6ca798b4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1.99)\n",
      "Requirement already satisfied: protobuf in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.24.4)\n",
      "Requirement already satisfied: gensim in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (6.0)\n",
      "Collecting torch>=1.10.0 (from accelerate)\n",
      "  Using cached torch-2.1.1-cp310-cp310-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (0.19.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Using cached torch-2.1.1-cp310-cp310-win_amd64.whl (192.3 MB)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: xgboost in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from xgboost) (1.26.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from xgboost) (1.11.4)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightgbm) (1.26.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from lightgbm) (1.11.4)\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.1.1)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.16.1%2Bcu121-cp310-cp310-win_amd64.whl (5.6 MB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.1.1%2Bcu121-cp310-cp310-win_amd64.whl (4.0 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: requests in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torch-2.1.1%2Bcu121-cp310-cp310-win_amd64.whl (2473.9 MB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.1\n",
      "    Uninstalling torch-2.1.1:\n",
      "      Successfully uninstalled torch-2.1.1\n",
      "Successfully installed torch-2.1.1+cu121 torchaudio-2.1.1+cu121 torchvision-0.16.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install protobuf\n",
    "!pip install gensim\n",
    "!pip install -U scikit-learn\n",
    "!pip install pandas\n",
    "!pip install -U accelerate\n",
    "!pip install -U transformers\n",
    "!pip install xgboost\n",
    "!pip install lightgbm\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362503eb",
   "metadata": {
    "id": "362503eb"
   },
   "source": [
    "# data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9ddf5d",
   "metadata": {
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1700910074571,
     "user": {
      "displayName": "스파르탄",
      "userId": "04034795070506982642"
     },
     "user_tz": -540
    },
    "id": "6a9ddf5d"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import dataclasses\n",
    "from typing import Iterable, Optional, Union, Dict, Tuple, List\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Sentence:\n",
    "    id: int\n",
    "    text: str\n",
    "    context: str\n",
    "    label: str\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Union[str, int]]:\n",
    "        return dataclasses.asdict(self)\n",
    "\n",
    "\n",
    "def balance_targets(sentences: Iterable[Sentence], method: str = \"downsample_o_cat\", shuffle=True) \\\n",
    "        -> Iterable[Sentence]:\n",
    "    \"\"\"\n",
    "    Oversamples and/or undersamples training sentences by a number of targets.\n",
    "    This is useful for linear shallow classifiers, that are prone to simply overfit the most-occurring category.\n",
    "    See the source code for a documentation of resample methods logic\n",
    "    :param shuffle: whether to shuffle the output\n",
    "    :param sentences: sentences to resample\n",
    "    :param method: resample method, one of {downsample_o_cat, downsample_o_pzk_cats, all_upsampled, remove_o_cat}\n",
    "    :return: resampled, possibly shuffled input sentences\n",
    "    \"\"\"\n",
    "    import random\n",
    "    # take the second-top count from categories apart from \"Other\"\n",
    "    targets = [s.label for s in sentences]\n",
    "    second_top_count = sorted([sum([target == cat for target in targets]) for cat in set(targets) - {\"O\"}])[-2]\n",
    "    if method == \"downsample_o_cat\":\n",
    "        # downsample \"other\" category to second-most-occurring category count\n",
    "        out_sentences = list((random.sample([s for s in sentences if s.label == \"O\"], second_top_count) +\n",
    "                         [s for s in sentences if s.label != \"O\"]))\n",
    "    elif method == \"downsample_o_pzk_cats\":\n",
    "        # downsample \"other\" + \"P_ZK\" (experience description) category to third-most-occurring category count\n",
    "        out_sentences = list((random.sample([s for s in sentences if s.label == \"O\"], second_top_count) +\n",
    "                         [s for s in sentences if s.label != \"O\"]))\n",
    "        out_sentences = list((random.sample([s for s in out_sentences if s.label == \"P_ZK\"], second_top_count) +\n",
    "                         [s for s in out_sentences if s.label != \"P_ZK\"]))\n",
    "    elif method == \"all_upsampled\":\n",
    "        # upsample all categories to a count of most-occurring one (presumably \"other\" category)\n",
    "        from itertools import chain\n",
    "        out_sentences = list(itertools.chain(*[random.choices([s for s in sentences if s.label == cat],\n",
    "                                                              k=second_top_count) for cat in set(targets)]))\n",
    "    elif method == \"remove_o_cat\":\n",
    "        # completely remove sentences of \"other\" category\n",
    "        out_sentences = [s for s in sentences if s.label != \"O\"]\n",
    "    else:\n",
    "        out_sentences = sentences\n",
    "    if shuffle:\n",
    "        # random shuffle output sentences\n",
    "        random.shuffle(out_sentences)\n",
    "    return out_sentences\n",
    "\n",
    "\n",
    "def get_sentence_vertical(sentences_dir: str, confidence_thrd: Optional[int] = 0) -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Creates a tab-separated csv table with sentences_text, tags, users and sources, in out_table_path\n",
    "    :param sentences_dir: directory of input sentences, divided to [train, val, test] subdirectories\n",
    "    :param confidence_thrd: minimal mean confidence threshold of the retrieved sentences\n",
    "    :return: Dataframe with attributes of retrieved sentences\n",
    "    \"\"\"\n",
    "    from itertools import chain\n",
    "    import pandas as pd  # if you need this, run 'pip install pandas==1.2.1'\n",
    "    #from utils.dataset import ReflexiveDataset\n",
    "\n",
    "    sentences_splits = [ReflexiveDataset.sentences_from_tsv(sentences_dir, dataset_type, confidence_thrd,\n",
    "                                                            use_context=True)\n",
    "                        for dataset_type in [\"train\", \"val\", \"test\"]]\n",
    "    out_vertical = pd.DataFrame.from_records([s.to_dict() for s in chain(*sentences_splits)])\n",
    "    return out_vertical\n",
    "\n",
    "\n",
    "def split_text_to_sentence_context(text: str, sep_chars: Tuple[str] = (\".\", \"?\", \"!\")) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Splits the input text to sentences with the corresponding context,\n",
    "    in the format compliant with the training of NeuralClassifier\n",
    "    :param text: Full input paragraph, e.g. whole reflective diary, to extract the sentences to classify\n",
    "    :param sep_chars: characters separating potential sentences\n",
    "    \"\"\"\n",
    "    out_sentences = []\n",
    "    current_sent = []\n",
    "    words = text.split()\n",
    "\n",
    "    for w_i, word in enumerate(words):\n",
    "        current_sent.append(word)\n",
    "        is_last_or_is_upper = (w_i == len(words)-1 or words[w_i+1][0].isupper())\n",
    "        if any(word.endswith(mark) for mark in sep_chars) and is_last_or_is_upper:\n",
    "            out_sentences.append(\" \".join(current_sent))\n",
    "            current_sent = []\n",
    "\n",
    "    for sent_i, sent in enumerate(out_sentences):\n",
    "        context = \" \".join(out_sentences[sent_i-2:sent_i+2])\n",
    "        yield sent, context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7e10ff",
   "metadata": {
    "id": "5a7e10ff"
   },
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52d2c9fc",
   "metadata": {
    "executionInfo": {
     "elapsed": 16913,
     "status": "ok",
     "timestamp": 1700910094751,
     "user": {
      "displayName": "스파르탄",
      "userId": "04034795070506982642"
     },
     "user_tz": -540
    },
    "id": "52d2c9fc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import List, Union, Dict\n",
    "import ast\n",
    "\n",
    "from filelock import FileLock\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer, InputFeatures, logging\n",
    "import pandas as pd\n",
    "# from .data_reader import Sentence\n",
    "\n",
    "logger = logging.get_logger()\n",
    "\n",
    "# original labels:\n",
    "# LABELS = [\"O\", \"OS_PRES\", \"PERS\", \"POC\", \"P_ZK\", \"REF_P\", \"UV_OBT\", \"VY_IN\", \"VY_VY\"]\n",
    "\n",
    "LABELS = [\"Other\", \"Belief\", \"Perspective\", \"Feeling\", \"Experience\",\n",
    "          \"Reflection\", \"Difficulty\", \"Intention\", \"Learning\", \"Difficulties\"]\n",
    "\n",
    "\n",
    "class Split(Enum):\n",
    "    train = \"train\"\n",
    "    eval = \"eval\"\n",
    "    test = \"test\"\n",
    "\n",
    "\n",
    "class ReflexiveDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sentences_dir: str, dataset_type: str, cache_dir: str, label_list: List[str],\n",
    "                 tokenizer: Union[AutoTokenizer, PreTrainedTokenizer],\n",
    "                 use_context=True, mean_confidence_threshold: int = 5):\n",
    "        self.sentences_dir = sentences_dir\n",
    "        self.confidence_thrd = mean_confidence_threshold\n",
    "        self.dataset_type = dataset_type\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_list = label_list\n",
    "        self.label_map = {label: i for i, label in enumerate(label_list)}\n",
    "        self.use_context = use_context\n",
    "\n",
    "        if not Path(cache_dir).exists():\n",
    "            Path(cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        cached_features_file = os.path.join(\n",
    "            cache_dir,\n",
    "            \"cached_{}_{}_{}\".format(dataset_type, tokenizer.__class__.__name__, str(self.tokenizer.model_max_length)),\n",
    "        )\n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "            logger.info(f\"Creating features from reflexive diaries\")\n",
    "            self.sentences = self.sentences_from_tsv(sentences_dir, dataset_type, self.confidence_thrd, self.use_context)\n",
    "            self.features = self.convert_examples_to_features(self.sentences)\n",
    "\n",
    "    @staticmethod\n",
    "    def sentences_from_tsv(sentences_dir: str, dataset_type: str,\n",
    "                           confidence_thrd: int, use_context: bool) -> List[Sentence]:\n",
    "        \"\"\"Creates sentences for the training, eval and test sets.\"\"\"\n",
    "        tsv_path = os.path.join(sentences_dir, dataset_type, \"sentences.tsv\")\n",
    "        df = pd.read_csv(tsv_path, sep='\\t')\n",
    "        df.sentence = df.sentence.fillna(\"\")\n",
    "        df.context = df.context.fillna(\"\")\n",
    "        sentences = []\n",
    "        # group by sources, iterate every group separately, to avoid context overlays\n",
    "        for idx, row in enumerate(df.itertuples()):\n",
    "            confidences = ast.literal_eval(row.confidence)\n",
    "            if sum(confidences) / len(confidences) >= confidence_thrd:\n",
    "                sentences.append(Sentence(id=row.idx, text=row.sentence,\n",
    "                                          context=row.context if use_context else None,\n",
    "                                          label=row.y))\n",
    "        logger.info(\"Retrieving %s of all %s %s sentences, over threshold %s\" %\n",
    "                    (len(sentences), len(df), dataset_type, confidence_thrd))\n",
    "        return sentences\n",
    "\n",
    "    def convert_examples_to_features(self, examples: List[Sentence]) -> List[Dict[str, List[int]]]:\n",
    "\n",
    "        batch_encoding = self.tokenizer(\n",
    "            text=[example.text.strip() for example in examples],\n",
    "            text_pair=[e.context.strip() for e in examples] if self.use_context else None,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        features = []\n",
    "        for i in range(len(examples)):\n",
    "            inputs = {k: batch_encoding[k][i] for k in batch_encoding if k != \"token_type_ids\"}\n",
    "            inputs[\"label\"] = self.label_map[examples[i].label]\n",
    "            features.append(inputs)\n",
    "\n",
    "        for i, example in enumerate(examples[:5]):\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"id: %s\" % (example.id))\n",
    "            logger.info(\"features: %s\" % features[i])\n",
    "\n",
    "        return features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i) -> InputFeatures:\n",
    "        return self.features[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eac5743",
   "metadata": {
    "id": "0eac5743"
   },
   "source": [
    "# shallow_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bccbde54",
   "metadata": {
    "executionInfo": {
     "elapsed": 762,
     "status": "ok",
     "timestamp": 1700910098779,
     "user": {
      "displayName": "스파르탄",
      "userId": "04034795070506982642"
     },
     "user_tz": -540
    },
    "id": "bccbde54"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from gensim import corpora\n",
    "from gensim import matutils\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#from .utils.dataset import ReflexiveDataset\n",
    "#from .utils.data_reader import Sentence\n",
    "\n",
    "\n",
    "class ShallowClassifier:\n",
    "    word_dictionary = None\n",
    "\n",
    "    def __init__(self, classifier, use_context: bool, bow_size: int,\n",
    "                 sentences_dir: Optional[str] = None, lang: str = \"cze\"):\n",
    "        self.classifier = classifier\n",
    "        self.sentences_dir = sentences_dir\n",
    "        self.use_context = use_context\n",
    "        self.bow_size = bow_size\n",
    "        self.lang = lang\n",
    "\n",
    "    def _preprocess_string(self, text: str) -> List[str]:\n",
    "        from gensim.parsing import preprocess_string\n",
    "        if self.lang == \"cze\":\n",
    "            from utils.cs_stemmer import cz_stem\n",
    "            return [cz_stem(word) for word in preprocess_string(text)]\n",
    "        else:\n",
    "            return preprocess_string(text)\n",
    "\n",
    "    def _initialize_bow_model(self, sents: List[Sentence]):\n",
    "        text_preprocessed = [self._preprocess_string(str(s.text)) for s in sents]\n",
    "        contexts_preprocessed = [self._preprocess_string(str(s.context)) for s in sents]\n",
    "\n",
    "        self.word_dictionary = corpora.Dictionary(text_preprocessed + contexts_preprocessed)\n",
    "        # keep most-occurring 10k words\n",
    "        # we need to check this with Ullmann\n",
    "        self.word_dictionary.filter_extremes(keep_n=self.bow_size)\n",
    "\n",
    "    def _vectorize_sentences(self, sents: List[Sentence]):\n",
    "        text_preprocessed = [self._preprocess_string(s.text) for s in sents]\n",
    "        # sparse matrix contains just pairs of co-occurrences\n",
    "        sparse_matrix = [self.word_dictionary.doc2bow(t) for t in text_preprocessed]\n",
    "        # we want to get natural, dense vectors for each document, containing the most-frequent num_terms\n",
    "        dense_matrix = matutils.corpus2dense(sparse_matrix, num_terms=self.bow_size).transpose()\n",
    "        if not self.use_context:\n",
    "            return dense_matrix\n",
    "        else:\n",
    "            # the same for contextual vectors\n",
    "            text_preprocessed_c = [self._preprocess_string(str(s.context)) for s in sents]\n",
    "            # sparse matrix contains just pairs of co-occurrences\n",
    "            sparse_matrix_c = [self.word_dictionary.doc2bow(t) for t in text_preprocessed_c]\n",
    "            # we want to get natural, dense vectors for each document, containing the most-frequent num_terms\n",
    "            dense_matrix_c = matutils.corpus2dense(sparse_matrix_c, num_terms=self.bow_size).transpose()\n",
    "\n",
    "            # concat textual and contextual vectors horizontally\n",
    "            return np.hstack([dense_matrix, dense_matrix_c])\n",
    "\n",
    "    def train(self, in_sentences: List[Sentence] = None, confidence_thrd: int = 5):\n",
    "        if in_sentences is None:\n",
    "            # get the dataset from outside\n",
    "            sentences = ReflexiveDataset.sentences_from_tsv(self.sentences_dir, \"train\",\n",
    "                                                            confidence_thrd, self.use_context)\n",
    "        else:\n",
    "            # user gets the dataset himself\n",
    "            sentences = in_sentences\n",
    "\n",
    "        self._initialize_bow_model(sentences)\n",
    "        vectors = self._vectorize_sentences(sentences)\n",
    "        self.classifier.fit(vectors, [s.label for s in in_sentences])\n",
    "\n",
    "    def predict(self, sentences: List[Sentence]):\n",
    "        vectors = self._vectorize_sentences(sentences)\n",
    "        targets = self.classifier.predict(vectors)\n",
    "        return targets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f5f770",
   "metadata": {
    "id": "07f5f770"
   },
   "source": [
    "# train_eval_shallow_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "227d3281",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7815,
     "status": "ok",
     "timestamp": 1700910109445,
     "user": {
      "displayName": "스파르탄",
      "userId": "04034795070506982642"
     },
     "user_tz": -540
    },
    "id": "227d3281",
    "outputId": "d404941d-4f67-49f7-c4f7-6a9719adc5a3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 149 sentences\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Belief       0.00      0.00      0.00         5\n",
      "Difficulties       0.00      0.00      0.00         6\n",
      "  Difficulty       0.00      0.00      0.00         0\n",
      "  Experience       0.50      0.06      0.11        31\n",
      "     Feeling       0.86      0.82      0.84        38\n",
      "    Learning       0.00      0.00      0.00         4\n",
      "       Other       0.52      0.90      0.66        59\n",
      "  Reflection       0.67      0.67      0.67         6\n",
      "\n",
      "    accuracy                           0.60       149\n",
      "   macro avg       0.32      0.31      0.29       149\n",
      "weighted avg       0.56      0.60      0.53       149\n",
      "\n",
      "0.6040268456375839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#from reflection_classification.utils.dataset import ReflexiveDataset\n",
    "#from reflection_classification.shallow_classifier import ShallowClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "#import argparse\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 하이퍼 파라미터\n",
    "    classifier = 'random_forrest' # 분류기 선택\n",
    "    sentences_dir = 'reflection-classification/data/sentences/en' # 데이터 위치\n",
    "    train_confidence_threshold = 5\n",
    "    test_confidence_threshold = 5\n",
    "    use_context = True\n",
    "    vocabulary_size = 800\n",
    "    language ='en'\n",
    "\n",
    "    # 분류기 설정\n",
    "    if classifier == 'random_forrest':\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        classifier = RandomForestClassifier()\n",
    "    elif classifier == 'logistic_regression':\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        classifier = LogisticRegression(max_iter=10e4)\n",
    "    elif classifier == 'naive_bayes':\n",
    "        from sklearn.naive_bayes import MultinomialNB\n",
    "        classifier = MultinomialNB()\n",
    "    elif classifier == 'support_vector_classifier':\n",
    "        from sklearn.svm import SVC\n",
    "        classifier = SVC()\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized classifier: %s\" % classifier)\n",
    "\n",
    "    train_sentences = ReflexiveDataset.sentences_from_tsv(sentences_dir, \"train\",\n",
    "                                                          train_confidence_threshold, use_context)\n",
    "    test_sentences = ReflexiveDataset.sentences_from_tsv(sentences_dir, \"test\",\n",
    "                                                         test_confidence_threshold, use_context)\n",
    "\n",
    "    cfr = ShallowClassifier(classifier=classifier, use_context=use_context, bow_size=vocabulary_size,\n",
    "                            lang=language)\n",
    "    cfr.train(train_sentences)\n",
    "    pred_targets = cfr.predict(test_sentences)\n",
    "    true_targets = [s.label for s in test_sentences]\n",
    "    objective_val = f1_score(true_targets, pred_targets, average='micro')\n",
    "    print(\"Evaluating on %s sentences\" % len(test_sentences))\n",
    "    print(\"Classification report: \\n%s\" % classification_report(true_targets, pred_targets))\n",
    "    print(objective_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c917a5",
   "metadata": {
    "id": "c3c917a5"
   },
   "source": [
    "# train_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f11f0179",
   "metadata": {
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1700910127709,
     "user": {
      "displayName": "스파르탄",
      "userId": "04034795070506982642"
     },
     "user_tz": -540
    },
    "id": "f11f0179"
   },
   "outputs": [],
   "source": [
    "#from .dataset import ReflexiveDataset\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import EvalPrediction, AutoTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def eval_fscore_acc(p: EvalPrediction) -> Dict:\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    acc = (preds == p.label_ids).mean()\n",
    "    f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='micro')\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"acc_and_f1\": (acc + f1) / 2,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_datasets(tokenizer: AutoTokenizer, sentences_dir: str, label_list: List[str],\n",
    "                 cache_dir: str, use_context: bool, confidence_thrd: int) -> List[ReflexiveDataset]:\n",
    "    return [ReflexiveDataset(sentences_dir, tokenizer=tokenizer, dataset_type=dataset_type, use_context=use_context,\n",
    "                             cache_dir=cache_dir, label_list=label_list, mean_confidence_threshold=confidence_thrd)\n",
    "            for dataset_type in [\"train\", \"val\", \"test\"]]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/transformer_config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d006a7",
   "metadata": {
    "id": "03d006a7"
   },
   "source": [
    "# train_neural_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73759b34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "executionInfo": {
     "elapsed": 896,
     "status": "error",
     "timestamp": 1700912300634,
     "user": {
      "displayName": "스파르탄",
      "userId": "04034795070506982642"
     },
     "user_tz": -540
    },
    "id": "73759b34",
    "outputId": "e1db2705-cc65-4739-cc26-91da92482f6d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [440/440 31:33, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Acc And F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.092000</td>\n",
       "      <td>1.144578</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.538614</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.886364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.499100</td>\n",
       "      <td>0.294260</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.245800</td>\n",
       "      <td>0.279922</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.127600</td>\n",
       "      <td>0.335063</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.086600</td>\n",
       "      <td>0.332919</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.059900</td>\n",
       "      <td>0.284524</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.340377</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating best model on test dataset: 100%|███████████████████████████████████████████| 41/41 [00:02<00:00, 19.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.926829268292683\n",
      "Trained model and training checkpoints are saved to /reflection-classification/models/bert\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, Trainer\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "#from reflection_classification.utils.train_utils import *\n",
    "#from reflection_classification.utils.dataset import LABELS\n",
    "\n",
    "# en_gtranslate thrd 5\n",
    "# Test accuracy: 0.7911392405063291\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # run this from /reflection-classification/reflection_classification\n",
    "\n",
    "    model_name = 'bert-base-multilingual-cased'\n",
    "    sentences_dir = 'reflection-classification/data/sentences/en'\n",
    "    trained_model_dir = '/reflection-classification/models/bert'\n",
    "    train_confidence_threshold=6\n",
    "    device = 'cuda' # cuda 환경을 만들어야 함\n",
    "    eval_on_test_set = True\n",
    "\n",
    "\n",
    "    model_args = ModelArguments(\n",
    "        model_name_or_path=model_name,\n",
    "    )\n",
    "\n",
    "    transformer_config = AutoConfig.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        num_labels=len(LABELS),\n",
    "        finetuning_task=\"classification\",\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, use_fast=False)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        config=transformer_config,\n",
    "    ).to(device)\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = get_datasets(tokenizer,\n",
    "                                                            cache_dir=trained_model_dir,\n",
    "                                                            label_list=LABELS,\n",
    "                                                            sentences_dir=sentences_dir,\n",
    "                                                            use_context=use_context,\n",
    "                                                            confidence_thrd=train_confidence_threshold)\n",
    "    training_args = TrainingArguments(output_dir=trained_model_dir,\n",
    "                                      overwrite_output_dir=True,\n",
    "                                      do_train=True,\n",
    "                                      do_eval=True,\n",
    "                                      do_predict=True,\n",
    "                                      per_device_train_batch_size=2,\n",
    "                                      per_device_eval_batch_size=2,\n",
    "                                      num_train_epochs=20,\n",
    "                                      warmup_steps=300,\n",
    "                                      logging_steps=50,\n",
    "                                      logging_first_step=True,\n",
    "                                      evaluation_strategy=\"steps\",\n",
    "                                      learning_rate=2e-5,\n",
    "                                      save_total_limit=16,\n",
    "                                      gradient_accumulation_steps=16,\n",
    "                                      load_best_model_at_end=True,\n",
    "                                      no_cuda=True if device == \"cpu\" else False,\n",
    "                                      metric_for_best_model=\"f1\")\n",
    "\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    trainer = Trainer(model=model,\n",
    "                      args=training_args,\n",
    "                      train_dataset=train_dataset,\n",
    "                      eval_dataset=val_dataset,\n",
    "                      compute_metrics=eval_fscore_acc,\n",
    "                      tokenizer=tokenizer,\n",
    "                      callbacks=[EarlyStoppingCallback(early_stopping_patience=10)])\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    if eval_on_test_set:\n",
    "        y_pred = [trainer.model(\n",
    "            **{k: torch.tensor(v).unsqueeze(0).to(trainer.model.device) for k, v in f.items() if k != 'label'},\n",
    "            return_dict=True).logits.argmax().item() for f in tqdm(test_dataset.features,\n",
    "                                                                   desc=\"Evaluating best model on test dataset\")]\n",
    "\n",
    "        y_trues = [f['label'] for f in test_dataset.features]\n",
    "\n",
    "        y_truepos = [y_trues[i] == y_pred[i] for i, _ in enumerate(y_pred)]\n",
    "\n",
    "        print(\"Test accuracy: %s\" % (sum(y_truepos) / len(y_truepos)))\n",
    "\n",
    "    trainer.save_model(trained_model_dir)\n",
    "    print(\"Trained model and training checkpoints are saved to %s\" % trained_model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e437871",
   "metadata": {
    "id": "9e437871"
   },
   "source": [
    "# neural_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "392b14fb",
   "metadata": {
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1700910174497,
     "user": {
      "displayName": "스파르탄",
      "userId": "04034795070506982642"
     },
     "user_tz": -540
    },
    "id": "392b14fb"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "#from .utils.dataset import LABELS\n",
    "\n",
    "\n",
    "class NeuralClassifier:\n",
    "\n",
    "    def __init__(self, model_path: str, uses_context: bool, device: str):\n",
    "        self.config = AutoConfig.from_pretrained(model_path)\n",
    "        self.device = device\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path, config=self.config).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.uses_context = uses_context\n",
    "\n",
    "    def predict_sentence(self, sentence: str, context: str = None):\n",
    "        if context is None and self.uses_context:\n",
    "            raise ValueError(\"You need to pass in context argument, including the sentence\")\n",
    "\n",
    "        features = self.tokenizer(sentence, text_pair=context,\n",
    "                                  padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "        outputs = self.model(**features.to(self.device), return_dict=True)\n",
    "        argmax = outputs.logits.argmax(dim=-1).detach().cpu().tolist()[0]\n",
    "        labels = LABELS[argmax]\n",
    "\n",
    "        return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d2e893",
   "metadata": {
    "id": "82d2e893"
   },
   "source": [
    "# eval_neural_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b9f8b06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11203,
     "status": "ok",
     "timestamp": 1700911953317,
     "user": {
      "displayName": "스파르탄",
      "userId": "04034795070506982642"
     },
     "user_tz": -540
    },
    "id": "4b9f8b06",
    "outputId": "1cbf3779-36de-4a20-f707-f6e40d533a71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 149/149 [00:20<00:00,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7785234899328859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from reflection_classification.utils.dataset import ReflexiveDataset\n",
    "#from reflection_classification.neural_classifier import NeuralClassifier\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #argparser = argparse.ArgumentParser()\n",
    "\n",
    "    sentences_dir='reflection-classification/data/sentences/en'\n",
    "    trained_model_dir='reflection-classification/models/roberta-large-nouda'\n",
    "    test_confidence_threshold=5\n",
    "    device = 'cuda'\n",
    "    use_context = True\n",
    "\n",
    "\n",
    "    classifier = NeuralClassifier(trained_model_dir, use_context, device)\n",
    "    test_sentences = ReflexiveDataset.sentences_from_tsv(sentences_dir, \"test\",\n",
    "                                                         test_confidence_threshold, use_context)\n",
    "\n",
    "    y_pred = [classifier.predict_sentence(sentence.text, sentence.context) for sentence in tqdm(test_sentences)]\n",
    "\n",
    "    y_trues = [sentence.label for sentence in test_sentences]\n",
    "\n",
    "    y_truepos = [y_trues[i] == y_pred[i] for i, _ in enumerate(y_pred)]\n",
    "\n",
    "    print(\"Test accuracy: %s\" % (sum(y_truepos) / len(y_truepos)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HOyNaQ1kSu7z",
   "metadata": {
    "id": "HOyNaQ1kSu7z"
   },
   "source": [
    "# 개선 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3Wnjecc0v5JM",
   "metadata": {
    "id": "3Wnjecc0v5JM"
   },
   "source": [
    "# NEW_train_eval_shallow_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "079d23a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11574,
     "status": "ok",
     "timestamp": 1700908613486,
     "user": {
      "displayName": "스파르탄",
      "userId": "04034795070506982642"
     },
     "user_tz": -540
    },
    "id": "079d23a2",
    "outputId": "dacba51b-5998-41fe-c060-2ad24a2da733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 149 sentences\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Belief       0.00      0.00      0.00         5\n",
      "Difficulties       0.00      0.00      0.00         6\n",
      "  Difficulty       0.00      0.00      0.00         0\n",
      "  Experience       0.50      0.10      0.16        31\n",
      "     Feeling       0.94      0.82      0.87        38\n",
      "    Learning       0.00      0.00      0.00         4\n",
      "       Other       0.55      0.95      0.70        59\n",
      "  Reflection       0.67      0.67      0.67         6\n",
      "\n",
      "    accuracy                           0.63       149\n",
      "   macro avg       0.33      0.32      0.30       149\n",
      "weighted avg       0.59      0.63      0.56       149\n",
      "\n",
      "0.6308724832214765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  F-score that is not between precision and recall.\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  F-score that is not between precision and recall.\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  F-score that is not between precision and recall.\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  F-score that is not between precision and recall.\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  F-score that is not between precision and recall.\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  F-score that is not between precision and recall.\n"
     ]
    }
   ],
   "source": [
    "#from reflection_classification.utils.dataset import ReflexiveDataset\n",
    "#from reflection_classification.shallow_classifier import ShallowClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "#import argparse\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 하이퍼 파라미터\n",
    "    classifier = 'random_forrest' # 분류기 선택\n",
    "    sentences_dir = 'reflection-classification/data/sentences/en' # 데이터 위치\n",
    "    train_confidence_threshold = 5\n",
    "    test_confidence_threshold = 5\n",
    "    use_context = True\n",
    "    vocabulary_size = 800\n",
    "    language ='en'\n",
    "\n",
    "    # 분류기 설정\n",
    "    if classifier == 'random_forrest':\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        classifier = RandomForestClassifier()\n",
    "    elif classifier == 'logistic_regression':\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        classifier = LogisticRegression(max_iter=10e4)\n",
    "    elif classifier == 'naive_bayes':\n",
    "        from sklearn.naive_bayes import MultinomialNB\n",
    "        classifier = MultinomialNB()\n",
    "    elif classifier == 'support_vector_classifier':\n",
    "        from sklearn.svm import SVC\n",
    "        classifier = SVC()\n",
    "    elif classifier == 'KNeighborsClassifier':\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        classifier = KNeighborsClassifier()\n",
    "    elif classifier == 'AdaBoostClassifier':\n",
    "        from sklearn.ensemble import AdaBoostClassifier\n",
    "        classifier = AdaBoostClassifier()\n",
    "    elif classifier == 'GradientBoostingClassifier':\n",
    "        from sklearn.ensemble import GradientBoostingClassifier\n",
    "        classifier = GradientBoostingClassifier()\n",
    "    elif classifier == 'XGBClassifier':\n",
    "        from xgboost import XGBClassifier\n",
    "        classifier = XGBClassifier()\n",
    "    elif classifier == 'LGBMClassifier':\n",
    "        from lightgbm import LGBMClassifier\n",
    "        classifier = LGBMClassifier()\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized classifier: %s\" % classifier)\n",
    "\n",
    "    train_sentences = ReflexiveDataset.sentences_from_tsv(sentences_dir, \"train\",\n",
    "                                                          train_confidence_threshold, use_context)\n",
    "    test_sentences = ReflexiveDataset.sentences_from_tsv(sentences_dir, \"test\",\n",
    "                                                         test_confidence_threshold, use_context)\n",
    "\n",
    "    cfr = ShallowClassifier(classifier=classifier, use_context=use_context, bow_size=vocabulary_size,\n",
    "                            lang=language)\n",
    "    cfr.train(train_sentences)\n",
    "    pred_targets = cfr.predict(test_sentences)\n",
    "    true_targets = [s.label for s in test_sentences]\n",
    "    objective_val = f1_score(true_targets, pred_targets, average='micro')\n",
    "    print(\"Evaluating on %s sentences\" % len(test_sentences))\n",
    "    print(\"Classification report: \\n%s\" % classification_report(true_targets, pred_targets))\n",
    "    print(objective_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FHnKX1atPpRI",
   "metadata": {
    "id": "FHnKX1atPpRI"
   },
   "source": [
    "# NEW_train_neural_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "mC3UtlZgPv2L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "executionInfo": {
     "elapsed": 750371,
     "status": "ok",
     "timestamp": 1700913226841,
     "user": {
      "displayName": "스파르탄",
      "userId": "04034795070506982642"
     },
     "user_tz": -540
    },
    "id": "mC3UtlZgPv2L",
    "outputId": "f37406e9-436c-4ff2-c758-0a7b0c5cb6db",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8c10d7ded94e9ab658df039be08f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c5048102d546de8ee6723659bba7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1155c1c0564c4e98bf3529be6dd7e212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30f7c315ae14ae39e479d28f19f56b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8026e1a5722444ecb7e03a5bb46fab56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [440/440 30:24, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Acc And F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.209200</td>\n",
       "      <td>1.620479</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.840909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.319700</td>\n",
       "      <td>0.805587</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.757600</td>\n",
       "      <td>0.427307</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.428800</td>\n",
       "      <td>0.290620</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.283116</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.164400</td>\n",
       "      <td>0.279489</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.363812</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.931818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.313221</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating best model on test dataset: 100%|███████████████████████████████████████████| 41/41 [00:02<00:00, 19.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9024390243902439\n",
      "Trained model and training checkpoints are saved to /reflection-classification/models/bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, Trainer\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "#from reflection_classification.utils.train_utils import *\n",
    "#from reflection_classification.utils.dataset import LABELS\n",
    "\n",
    "# en_gtranslate thrd 5\n",
    "# Test accuracy: 0.7911392405063291\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # run this from /reflection-classification/reflection_classification\n",
    "\n",
    "    model_name = 'bert-base-uncased'\n",
    "    sentences_dir = 'reflection-classification/data/sentences/en'\n",
    "    trained_model_dir = '/reflection-classification/models/bert-base-uncased'\n",
    "    train_confidence_threshold=6\n",
    "    device = 'cuda' # cuda 환경을 만들어야 함\n",
    "    eval_on_test_set = True\n",
    "\n",
    "\n",
    "    model_args = ModelArguments(\n",
    "        model_name_or_path=model_name,\n",
    "    )\n",
    "\n",
    "    transformer_config = AutoConfig.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        num_labels=len(LABELS),\n",
    "        finetuning_task=\"classification\",\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, use_fast=False)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        config=transformer_config,\n",
    "    ).to(device)\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = get_datasets(tokenizer,\n",
    "                                                            cache_dir=trained_model_dir,\n",
    "                                                            label_list=LABELS,\n",
    "                                                            sentences_dir=sentences_dir,\n",
    "                                                            use_context=use_context,\n",
    "                                                            confidence_thrd=train_confidence_threshold)\n",
    "    training_args = TrainingArguments(output_dir=trained_model_dir,\n",
    "                                      overwrite_output_dir=True,\n",
    "                                      do_train=True,\n",
    "                                      do_eval=True,\n",
    "                                      do_predict=True,\n",
    "                                      per_device_train_batch_size=2,\n",
    "                                      per_device_eval_batch_size=2,\n",
    "                                      num_train_epochs=20,\n",
    "                                      warmup_steps=300,\n",
    "                                      logging_steps=50,\n",
    "                                      logging_first_step=True,\n",
    "                                      evaluation_strategy=\"steps\",\n",
    "                                      learning_rate=2e-5,\n",
    "                                      save_total_limit=16,\n",
    "                                      gradient_accumulation_steps=16,\n",
    "                                      load_best_model_at_end=True,\n",
    "                                      no_cuda=True if device == \"cpu\" else False,\n",
    "                                      metric_for_best_model=\"f1\")\n",
    "\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    trainer = Trainer(model=model,\n",
    "                      args=training_args,\n",
    "                      train_dataset=train_dataset,\n",
    "                      eval_dataset=val_dataset,\n",
    "                      compute_metrics=eval_fscore_acc,\n",
    "                      tokenizer=tokenizer,\n",
    "                      callbacks=[EarlyStoppingCallback(early_stopping_patience=10)])\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    if eval_on_test_set:\n",
    "        y_pred = [trainer.model(\n",
    "            **{k: torch.tensor(v).unsqueeze(0).to(trainer.model.device) for k, v in f.items() if k != 'label'},\n",
    "            return_dict=True).logits.argmax().item() for f in tqdm(test_dataset.features,\n",
    "                                                                   desc=\"Evaluating best model on test dataset\")]\n",
    "\n",
    "        y_trues = [f['label'] for f in test_dataset.features]\n",
    "\n",
    "        y_truepos = [y_trues[i] == y_pred[i] for i, _ in enumerate(y_pred)]\n",
    "\n",
    "        print(\"Test accuracy: %s\" % (sum(y_truepos) / len(y_truepos)))\n",
    "\n",
    "    trainer.save_model(trained_model_dir)\n",
    "    print(\"Trained model and training checkpoints are saved to %s\" % trained_model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O-kc-jxUS8g8",
   "metadata": {
    "id": "O-kc-jxUS8g8"
   },
   "source": [
    "# eval_neural_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ajiTmBRRS7q8",
   "metadata": {
    "id": "ajiTmBRRS7q8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 149/149 [00:07<00:00, 20.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7516778523489933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from reflection_classification.utils.dataset import ReflexiveDataset\n",
    "#from reflection_classification.neural_classifier import NeuralClassifier\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #argparser = argparse.ArgumentParser()\n",
    "\n",
    "    sentences_dir='reflection-classification/data/sentences/en'\n",
    "    trained_model_dir='/reflection-classification/models/bert-base-uncased'\n",
    "    test_confidence_threshold=5\n",
    "    device = 'cuda'\n",
    "    use_context = True\n",
    "\n",
    "\n",
    "    classifier = NeuralClassifier(trained_model_dir, use_context, device)\n",
    "    test_sentences = ReflexiveDataset.sentences_from_tsv(sentences_dir, \"test\",\n",
    "                                                         test_confidence_threshold, use_context)\n",
    "\n",
    "    y_pred = [classifier.predict_sentence(sentence.text, sentence.context) for sentence in tqdm(test_sentences)]\n",
    "\n",
    "    y_trues = [sentence.label for sentence in test_sentences]\n",
    "\n",
    "    y_truepos = [y_trues[i] == y_pred[i] for i, _ in enumerate(y_pred)]\n",
    "\n",
    "    print(\"Test accuracy: %s\" % (sum(y_truepos) / len(y_truepos)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cefe59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
