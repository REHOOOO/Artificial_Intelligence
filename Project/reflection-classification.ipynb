{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9acf28c",
   "metadata": {},
   "source": [
    "# install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "602dc7ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 123.5/123.5 kB 7.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.10.3-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp310-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.0-cp310-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.1/7.9 MB 34.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.5/7.9 MB 32.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.3/7.9 MB 39.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.7/7.9 MB 40.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.9/7.9 MB 40.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 38.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 36.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 311.7/311.7 kB ? eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp310-cp310-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 269.6/269.6 kB ? eta 0:00:00\n",
      "Downloading safetensors-0.4.0-cp310-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 277.4/277.4 kB 16.7 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 1.8/2.2 MB 58.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 46.6 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 4.3 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.19.4 regex-2023.10.3 safetensors-0.4.0 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-win_amd64.whl (977 kB)\n",
      "     ---------------------------------------- 0.0/977.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 977.5/977.5 kB 31.2 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "Requirement already satisfied: protobuf in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.24.4)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp310-cp310-win_amd64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.11.4)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading gensim-4.3.2-cp310-cp310-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.7/24.0 MB 53.8 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 3.5/24.0 MB 44.6 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 5.3/24.0 MB 48.6 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 6.6/24.0 MB 46.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 8.1/24.0 MB 43.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 9.4/24.0 MB 42.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 10.7/24.0 MB 43.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.1/24.0 MB 43.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 13.5/24.0 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.9/24.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 16.3/24.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 17.6/24.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.2/24.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 20.6/24.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.0/24.0 MB 40.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.3/24.0 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 34.4 MB/s eta 0:00:00\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.0/57.0 kB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-6.4.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install protobuf\n",
    "!pip install gensim\n",
    "!pip install -U scikit-learn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362503eb",
   "metadata": {},
   "source": [
    "# data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a9ddf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import dataclasses\n",
    "from typing import Iterable, Optional, Union, Dict, Tuple, List\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Sentence:\n",
    "    id: int\n",
    "    text: str\n",
    "    context: str\n",
    "    label: str\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Union[str, int]]:\n",
    "        return dataclasses.asdict(self)\n",
    "\n",
    "\n",
    "def balance_targets(sentences: Iterable[Sentence], method: str = \"downsample_o_cat\", shuffle=True) \\\n",
    "        -> Iterable[Sentence]:\n",
    "    \"\"\"\n",
    "    Oversamples and/or undersamples training sentences by a number of targets.\n",
    "    This is useful for linear shallow classifiers, that are prone to simply overfit the most-occurring category.\n",
    "    See the source code for a documentation of resample methods logic\n",
    "    :param shuffle: whether to shuffle the output\n",
    "    :param sentences: sentences to resample\n",
    "    :param method: resample method, one of {downsample_o_cat, downsample_o_pzk_cats, all_upsampled, remove_o_cat}\n",
    "    :return: resampled, possibly shuffled input sentences\n",
    "    \"\"\"\n",
    "    import random\n",
    "    # take the second-top count from categories apart from \"Other\"\n",
    "    targets = [s.label for s in sentences]\n",
    "    second_top_count = sorted([sum([target == cat for target in targets]) for cat in set(targets) - {\"O\"}])[-2]\n",
    "    if method == \"downsample_o_cat\":\n",
    "        # downsample \"other\" category to second-most-occurring category count\n",
    "        out_sentences = list((random.sample([s for s in sentences if s.label == \"O\"], second_top_count) +\n",
    "                         [s for s in sentences if s.label != \"O\"]))\n",
    "    elif method == \"downsample_o_pzk_cats\":\n",
    "        # downsample \"other\" + \"P_ZK\" (experience description) category to third-most-occurring category count\n",
    "        out_sentences = list((random.sample([s for s in sentences if s.label == \"O\"], second_top_count) +\n",
    "                         [s for s in sentences if s.label != \"O\"]))\n",
    "        out_sentences = list((random.sample([s for s in out_sentences if s.label == \"P_ZK\"], second_top_count) +\n",
    "                         [s for s in out_sentences if s.label != \"P_ZK\"]))\n",
    "    elif method == \"all_upsampled\":\n",
    "        # upsample all categories to a count of most-occurring one (presumably \"other\" category)\n",
    "        from itertools import chain\n",
    "        out_sentences = list(itertools.chain(*[random.choices([s for s in sentences if s.label == cat],\n",
    "                                                              k=second_top_count) for cat in set(targets)]))\n",
    "    elif method == \"remove_o_cat\":\n",
    "        # completely remove sentences of \"other\" category\n",
    "        out_sentences = [s for s in sentences if s.label != \"O\"]\n",
    "    else:\n",
    "        out_sentences = sentences\n",
    "    if shuffle:\n",
    "        # random shuffle output sentences\n",
    "        random.shuffle(out_sentences)\n",
    "    return out_sentences\n",
    "\n",
    "\n",
    "def get_sentence_vertical(sentences_dir: str, confidence_thrd: Optional[int] = 0) -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Creates a tab-separated csv table with sentences_text, tags, users and sources, in out_table_path\n",
    "    :param sentences_dir: directory of input sentences, divided to [train, val, test] subdirectories\n",
    "    :param confidence_thrd: minimal mean confidence threshold of the retrieved sentences\n",
    "    :return: Dataframe with attributes of retrieved sentences\n",
    "    \"\"\"\n",
    "    from itertools import chain\n",
    "    import pandas as pd  # if you need this, run 'pip install pandas==1.2.1'\n",
    "    from utils.dataset import ReflexiveDataset\n",
    "\n",
    "    sentences_splits = [ReflexiveDataset.sentences_from_tsv(sentences_dir, dataset_type, confidence_thrd,\n",
    "                                                            use_context=True)\n",
    "                        for dataset_type in [\"train\", \"val\", \"test\"]]\n",
    "    out_vertical = pd.DataFrame.from_records([s.to_dict() for s in chain(*sentences_splits)])\n",
    "    return out_vertical\n",
    "\n",
    "\n",
    "def split_text_to_sentence_context(text: str, sep_chars: Tuple[str] = (\".\", \"?\", \"!\")) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Splits the input text to sentences with the corresponding context,\n",
    "    in the format compliant with the training of NeuralClassifier\n",
    "    :param text: Full input paragraph, e.g. whole reflective diary, to extract the sentences to classify\n",
    "    :param sep_chars: characters separating potential sentences\n",
    "    \"\"\"\n",
    "    out_sentences = []\n",
    "    current_sent = []\n",
    "    words = text.split()\n",
    "\n",
    "    for w_i, word in enumerate(words):\n",
    "        current_sent.append(word)\n",
    "        is_last_or_is_upper = (w_i == len(words)-1 or words[w_i+1][0].isupper())\n",
    "        if any(word.endswith(mark) for mark in sep_chars) and is_last_or_is_upper:\n",
    "            out_sentences.append(\" \".join(current_sent))\n",
    "            current_sent = []\n",
    "\n",
    "    for sent_i, sent in enumerate(out_sentences):\n",
    "        context = \" \".join(out_sentences[sent_i-2:sent_i+2])\n",
    "        yield sent, context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7e10ff",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52d2c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import List, Union, Dict\n",
    "import ast\n",
    "\n",
    "from filelock import FileLock\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer, InputFeatures, logging\n",
    "import pandas as pd\n",
    "# from .data_reader import Sentence\n",
    "\n",
    "logger = logging.get_logger()\n",
    "\n",
    "# original labels:\n",
    "# LABELS = [\"O\", \"OS_PRES\", \"PERS\", \"POC\", \"P_ZK\", \"REF_P\", \"UV_OBT\", \"VY_IN\", \"VY_VY\"]\n",
    "\n",
    "LABELS = [\"Other\", \"Belief\", \"Perspective\", \"Feeling\", \"Experience\",\n",
    "          \"Reflection\", \"Difficulty\", \"Intention\", \"Learning\"]\n",
    "\n",
    "\n",
    "class Split(Enum):\n",
    "    train = \"train\"\n",
    "    eval = \"eval\"\n",
    "    test = \"test\"\n",
    "\n",
    "\n",
    "class ReflexiveDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, sentences_dir: str, dataset_type: str, cache_dir: str, label_list: List[str],\n",
    "                 tokenizer: Union[AutoTokenizer, PreTrainedTokenizer],\n",
    "                 use_context=True, mean_confidence_threshold: int = 5):\n",
    "        self.sentences_dir = sentences_dir\n",
    "        self.confidence_thrd = mean_confidence_threshold\n",
    "        self.dataset_type = dataset_type\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_list = label_list\n",
    "        self.label_map = {label: i for i, label in enumerate(label_list)}\n",
    "        self.use_context = use_context\n",
    "\n",
    "        if not Path(cache_dir).exists():\n",
    "            Path(cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        cached_features_file = os.path.join(\n",
    "            cache_dir,\n",
    "            \"cached_{}_{}_{}\".format(dataset_type, tokenizer.__class__.__name__, str(self.tokenizer.model_max_length)),\n",
    "        )\n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "            logger.info(f\"Creating features from reflexive diaries\")\n",
    "            self.sentences = self.sentences_from_tsv(sentences_dir, dataset_type, self.confidence_thrd, self.use_context)\n",
    "            self.features = self.convert_examples_to_features(self.sentences)\n",
    "\n",
    "    @staticmethod\n",
    "    def sentences_from_tsv(sentences_dir: str, dataset_type: str,\n",
    "                           confidence_thrd: int, use_context: bool) -> List[Sentence]:\n",
    "        \"\"\"Creates sentences for the training, eval and test sets.\"\"\"\n",
    "        tsv_path = os.path.join(sentences_dir, dataset_type, \"sentences.tsv\")\n",
    "        df = pd.read_csv(tsv_path, sep='\\t')\n",
    "        df.sentence = df.sentence.fillna(\"\")\n",
    "        df.context = df.context.fillna(\"\")\n",
    "        sentences = []\n",
    "        # group by sources, iterate every group separately, to avoid context overlays\n",
    "        for idx, row in enumerate(df.itertuples()):\n",
    "            confidences = ast.literal_eval(row.confidence)\n",
    "            if sum(confidences) / len(confidences) >= confidence_thrd:\n",
    "                sentences.append(Sentence(id=row.idx, text=row.sentence,\n",
    "                                          context=row.context if use_context else None,\n",
    "                                          label=row.y))\n",
    "        logger.info(\"Retrieving %s of all %s %s sentences, over threshold %s\" %\n",
    "                    (len(sentences), len(df), dataset_type, confidence_thrd))\n",
    "        return sentences\n",
    "    \n",
    "    def convert_examples_to_features(self, examples: List[Sentence]) -> List[Dict[str, List[int]]]:\n",
    "\n",
    "        batch_encoding = self.tokenizer(\n",
    "            text=[example.text.strip() for example in examples],\n",
    "            text_pair=[e.context.strip() for e in examples] if self.use_context else None,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        features = []\n",
    "        for i in range(len(examples)):\n",
    "            inputs = {k: batch_encoding[k][i] for k in batch_encoding if k != \"token_type_ids\"}\n",
    "            inputs[\"label\"] = self.label_map[examples[i].label]\n",
    "            features.append(inputs)\n",
    "\n",
    "        for i, example in enumerate(examples[:5]):\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"id: %s\" % (example.id))\n",
    "            logger.info(\"features: %s\" % features[i])\n",
    "\n",
    "        return features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i) -> InputFeatures:\n",
    "        return self.features[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eac5743",
   "metadata": {},
   "source": [
    "# shallow_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bccbde54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from gensim import corpora\n",
    "from gensim import matutils\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#from .utils.dataset import ReflexiveDataset\n",
    "#from .utils.data_reader import Sentence\n",
    "\n",
    "\n",
    "class ShallowClassifier:\n",
    "    word_dictionary = None\n",
    "\n",
    "    def __init__(self, classifier, use_context: bool, bow_size: int,\n",
    "                 sentences_dir: Optional[str] = None, lang: str = \"cze\"):\n",
    "        self.classifier = classifier\n",
    "        self.sentences_dir = sentences_dir\n",
    "        self.use_context = use_context\n",
    "        self.bow_size = bow_size\n",
    "        self.lang = lang\n",
    "\n",
    "    def _preprocess_string(self, text: str) -> List[str]:\n",
    "        from gensim.parsing import preprocess_string\n",
    "        if self.lang == \"cze\":\n",
    "            from utils.cs_stemmer import cz_stem\n",
    "            return [cz_stem(word) for word in preprocess_string(text)]\n",
    "        else:\n",
    "            return preprocess_string(text)\n",
    "\n",
    "    def _initialize_bow_model(self, sents: List[Sentence]):\n",
    "        text_preprocessed = [self._preprocess_string(str(s.text)) for s in sents]\n",
    "        contexts_preprocessed = [self._preprocess_string(str(s.context)) for s in sents]\n",
    "\n",
    "        self.word_dictionary = corpora.Dictionary(text_preprocessed + contexts_preprocessed)\n",
    "        # keep most-occurring 10k words\n",
    "        # we need to check this with Ullmann\n",
    "        self.word_dictionary.filter_extremes(keep_n=self.bow_size)\n",
    "\n",
    "    def _vectorize_sentences(self, sents: List[Sentence]):\n",
    "        text_preprocessed = [self._preprocess_string(s.text) for s in sents]\n",
    "        # sparse matrix contains just pairs of co-occurrences\n",
    "        sparse_matrix = [self.word_dictionary.doc2bow(t) for t in text_preprocessed]\n",
    "        # we want to get natural, dense vectors for each document, containing the most-frequent num_terms\n",
    "        dense_matrix = matutils.corpus2dense(sparse_matrix, num_terms=self.bow_size).transpose()\n",
    "        if not self.use_context:\n",
    "            return dense_matrix\n",
    "        else:\n",
    "            # the same for contextual vectors\n",
    "            text_preprocessed_c = [self._preprocess_string(str(s.context)) for s in sents]\n",
    "            # sparse matrix contains just pairs of co-occurrences\n",
    "            sparse_matrix_c = [self.word_dictionary.doc2bow(t) for t in text_preprocessed_c]\n",
    "            # we want to get natural, dense vectors for each document, containing the most-frequent num_terms\n",
    "            dense_matrix_c = matutils.corpus2dense(sparse_matrix_c, num_terms=self.bow_size).transpose()\n",
    "\n",
    "            # concat textual and contextual vectors horizontally\n",
    "            return np.hstack([dense_matrix, dense_matrix_c])\n",
    "\n",
    "    def train(self, in_sentences: List[Sentence] = None, confidence_thrd: int = 5):\n",
    "        if in_sentences is None:\n",
    "            # get the dataset from outside\n",
    "            sentences = ReflexiveDataset.sentences_from_tsv(self.sentences_dir, \"train\",\n",
    "                                                            confidence_thrd, self.use_context)\n",
    "        else:\n",
    "            # user gets the dataset himself\n",
    "            sentences = in_sentences\n",
    "\n",
    "        self._initialize_bow_model(sentences)\n",
    "        vectors = self._vectorize_sentences(sentences)\n",
    "        self.classifier.fit(vectors, [s.label for s in in_sentences])\n",
    "\n",
    "    def predict(self, sentences: List[Sentence]):\n",
    "        vectors = self._vectorize_sentences(sentences)\n",
    "        targets = self.classifier.predict(vectors)\n",
    "        return targets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f5f770",
   "metadata": {},
   "source": [
    "# train_eval_shallow_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "227d3281",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'add_argument'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      9\u001b[0m     argparser \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--classifier=random_forrest \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m --sentences_dir=reflection-classification/data/sentences/en \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m--train_confidence_threshold=5 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m--test_confidence_threshold=5 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m--use_context=True \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m--vocabulary_size=800\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m     \u001b[43margparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_argument\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--classifier\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     12\u001b[0m                            help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClassifier to use. One of: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mrandom_forrest, logistic_regression, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnaive_bayes, support_vector_classifier}\u001b[39m\u001b[38;5;124m'\u001b[39m, required\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m     argparser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--sentences_dir\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, required\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m                            help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDirectory with \u001b[39m\u001b[38;5;132;01m{split}\u001b[39;00m\u001b[38;5;124m/sentence.tsv of annotated sentences\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m     argparser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--train_confidence_threshold\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m     17\u001b[0m                            help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal confidence threshold for sentences to train on.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     18\u001b[0m                            default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'add_argument'"
     ]
    }
   ],
   "source": [
    "#from reflection_classification.utils.dataset import ReflexiveDataset\n",
    "#from reflection_classification.shallow_classifier import ShallowClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import argparse\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    argparser = \"--classifier=random_forrest \\ --sentences_dir=reflection-classification/data/sentences/en \\--train_confidence_threshold=5 \\--test_confidence_threshold=5 \\--use_context=True \\--vocabulary_size=800\"\n",
    "\n",
    "    argparser.add_argument('--classifier', type=str,\n",
    "                           help='Classifier to use. One of: {random_forrest, logistic_regression, '\n",
    "                                'naive_bayes, support_vector_classifier}', required=True)\n",
    "    argparser.add_argument('--sentences_dir', type=str, required=True,\n",
    "                           help='Directory with {split}/sentence.tsv of annotated sentences')\n",
    "    argparser.add_argument('--train_confidence_threshold', type=int,\n",
    "                           help='Minimal confidence threshold for sentences to train on.',\n",
    "                           default=5)\n",
    "    argparser.add_argument('--test_confidence_threshold', type=int,\n",
    "                           help='Minimal confidence threshold for sentences to test on.',\n",
    "                           default=5)\n",
    "    argparser.add_argument('--use_context', type=bool, help='Whether the model was trainer using context.',\n",
    "                           default=True)\n",
    "    argparser.add_argument('--language', type=str, help='Language to decide on how to remove stemming.',\n",
    "                           default=\"en\")\n",
    "    argparser.add_argument('--vocabulary_size', type=int,\n",
    "                           help='Number of top-n most-occurring words used '\n",
    "                                'to create Bag of Words representation for classification',\n",
    "                           default=300)\n",
    "    args = argparser.parse_args()\n",
    "\n",
    "    if args.classifier == 'random_forrest':\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        classifier = RandomForestClassifier()\n",
    "    elif args.classifier == 'logistic_regression':\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        classifier = LogisticRegression(max_iter=10e4)\n",
    "    elif args.classifier == 'naive_bayes':\n",
    "        from sklearn.naive_bayes import MultinomialNB\n",
    "        classifier = MultinomialNB()\n",
    "    elif args.classifier == 'support_vector_classifier':\n",
    "        from sklearn.svm import SVC\n",
    "        classifier = SVC()\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized classifier: %s\" % args.classifier)\n",
    "\n",
    "    train_sentences = ReflexiveDataset.sentences_from_tsv(args.sentences_dir, \"train\",\n",
    "                                                          args.train_confidence_threshold, args.use_context)\n",
    "    test_sentences = ReflexiveDataset.sentences_from_tsv(args.sentences_dir, \"test\",\n",
    "                                                         args.test_confidence_threshold, args.use_context)\n",
    "\n",
    "    cfr = ShallowClassifier(classifier=classifier, use_context=args.use_context, bow_size=args.vocabulary_size,\n",
    "                            lang=args.language)\n",
    "    cfr.train(train_sentences)\n",
    "    pred_targets = cfr.predict(test_sentences)\n",
    "    true_targets = [s.label for s in test_sentences]\n",
    "    objective_val = f1_score(true_targets, pred_targets, average='micro')\n",
    "    print(\"Evaluating on %s sentences\" % len(test_sentences))\n",
    "    print(\"Classification report: \\n%s\" % classification_report(true_targets, pred_targets))\n",
    "    print(objective_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f761abe",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 30\u001b[0m\n\u001b[0;32m     24\u001b[0m argparser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--language\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLanguage to decide on how to remove stemming.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     25\u001b[0m                        default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m argparser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--vocabulary_size\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m     27\u001b[0m                        help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of top-n most-occurring words used \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     28\u001b[0m                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto create Bag of Words representation for classification\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     29\u001b[0m                        default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43margparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_forrest\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\argparse.py:1826\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1826\u001b[0m     args, argv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m argv:\n\u001b[0;32m   1828\u001b[0m         msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munrecognized arguments: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\argparse.py:1859\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexit_on_error:\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1859\u001b[0m         namespace, args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ArgumentError:\n\u001b[0;32m   1861\u001b[0m         err \u001b[38;5;241m=\u001b[39m _sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\argparse.py:2100\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args\u001b[1;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[0;32m   2096\u001b[0m                 \u001b[38;5;28msetattr\u001b[39m(namespace, action\u001b[38;5;241m.\u001b[39mdest,\n\u001b[0;32m   2097\u001b[0m                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(action, action\u001b[38;5;241m.\u001b[39mdefault))\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_actions:\n\u001b[1;32m-> 2100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthe following arguments are required: \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\n\u001b[0;32m   2101\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequired_actions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[38;5;66;03m# make sure all required groups had one option present\u001b[39;00m\n\u001b[0;32m   2104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutually_exclusive_groups:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\argparse.py:2587\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2585\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_usage(_sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m   2586\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[1;32m-> 2587\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\argparse.py:2574\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message:\n\u001b[0;32m   2573\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m-> 2574\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73759b34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
