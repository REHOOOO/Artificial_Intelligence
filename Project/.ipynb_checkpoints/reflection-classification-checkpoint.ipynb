{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9acf28c",
   "metadata": {},
   "source": [
    "# install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "602dc7ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 123.5/123.5 kB 7.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.10.3-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp310-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.0-cp310-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.1/7.9 MB 34.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.5/7.9 MB 32.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.3/7.9 MB 39.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.7/7.9 MB 40.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.9/7.9 MB 40.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 38.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 36.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 311.7/311.7 kB ? eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp310-cp310-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 269.6/269.6 kB ? eta 0:00:00\n",
      "Downloading safetensors-0.4.0-cp310-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 277.4/277.4 kB 16.7 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 1.8/2.2 MB 58.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 46.6 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 4.3 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.19.4 regex-2023.10.3 safetensors-0.4.0 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-win_amd64.whl (977 kB)\n",
      "     ---------------------------------------- 0.0/977.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 977.5/977.5 kB 31.2 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "Requirement already satisfied: protobuf in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.24.4)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp310-cp310-win_amd64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.11.4)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading gensim-4.3.2-cp310-cp310-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.7/24.0 MB 53.8 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 3.5/24.0 MB 44.6 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 5.3/24.0 MB 48.6 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 6.6/24.0 MB 46.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 8.1/24.0 MB 43.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 9.4/24.0 MB 42.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 10.7/24.0 MB 43.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.1/24.0 MB 43.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 13.5/24.0 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.9/24.0 MB 38.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 16.3/24.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 17.6/24.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.2/24.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 20.6/24.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.0/24.0 MB 40.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.3/24.0 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.0 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 34.4 MB/s eta 0:00:00\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.0/57.0 kB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-6.4.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ryu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install protobuf\n",
    "!pip install gensim\n",
    "!pip install -U scikit-learn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362503eb",
   "metadata": {},
   "source": [
    "# data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9ddf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import dataclasses\n",
    "from typing import Iterable, Optional, Union, Dict, Tuple, List\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Sentence:\n",
    "    id: int\n",
    "    text: str\n",
    "    context: str\n",
    "    label: str\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Union[str, int]]:\n",
    "        return dataclasses.asdict(self)\n",
    "\n",
    "\n",
    "def balance_targets(sentences: Iterable[Sentence], method: str = \"downsample_o_cat\", shuffle=True) \\\n",
    "        -> Iterable[Sentence]:\n",
    "    \"\"\"\n",
    "    Oversamples and/or undersamples training sentences by a number of targets.\n",
    "    This is useful for linear shallow classifiers, that are prone to simply overfit the most-occurring category.\n",
    "    See the source code for a documentation of resample methods logic\n",
    "    :param shuffle: whether to shuffle the output\n",
    "    :param sentences: sentences to resample\n",
    "    :param method: resample method, one of {downsample_o_cat, downsample_o_pzk_cats, all_upsampled, remove_o_cat}\n",
    "    :return: resampled, possibly shuffled input sentences\n",
    "    \"\"\"\n",
    "    import random\n",
    "    # take the second-top count from categories apart from \"Other\"\n",
    "    targets = [s.label for s in sentences]\n",
    "    second_top_count = sorted([sum([target == cat for target in targets]) for cat in set(targets) - {\"O\"}])[-2]\n",
    "    if method == \"downsample_o_cat\":\n",
    "        # downsample \"other\" category to second-most-occurring category count\n",
    "        out_sentences = list((random.sample([s for s in sentences if s.label == \"O\"], second_top_count) +\n",
    "                         [s for s in sentences if s.label != \"O\"]))\n",
    "    elif method == \"downsample_o_pzk_cats\":\n",
    "        # downsample \"other\" + \"P_ZK\" (experience description) category to third-most-occurring category count\n",
    "        out_sentences = list((random.sample([s for s in sentences if s.label == \"O\"], second_top_count) +\n",
    "                         [s for s in sentences if s.label != \"O\"]))\n",
    "        out_sentences = list((random.sample([s for s in out_sentences if s.label == \"P_ZK\"], second_top_count) +\n",
    "                         [s for s in out_sentences if s.label != \"P_ZK\"]))\n",
    "    elif method == \"all_upsampled\":\n",
    "        # upsample all categories to a count of most-occurring one (presumably \"other\" category)\n",
    "        from itertools import chain\n",
    "        out_sentences = list(itertools.chain(*[random.choices([s for s in sentences if s.label == cat],\n",
    "                                                              k=second_top_count) for cat in set(targets)]))\n",
    "    elif method == \"remove_o_cat\":\n",
    "        # completely remove sentences of \"other\" category\n",
    "        out_sentences = [s for s in sentences if s.label != \"O\"]\n",
    "    else:\n",
    "        out_sentences = sentences\n",
    "    if shuffle:\n",
    "        # random shuffle output sentences\n",
    "        random.shuffle(out_sentences)\n",
    "    return out_sentences\n",
    "\n",
    "\n",
    "def get_sentence_vertical(sentences_dir: str, confidence_thrd: Optional[int] = 0) -> 'DataFrame':\n",
    "    \"\"\"\n",
    "    Creates a tab-separated csv table with sentences_text, tags, users and sources, in out_table_path\n",
    "    :param sentences_dir: directory of input sentences, divided to [train, val, test] subdirectories\n",
    "    :param confidence_thrd: minimal mean confidence threshold of the retrieved sentences\n",
    "    :return: Dataframe with attributes of retrieved sentences\n",
    "    \"\"\"\n",
    "    from itertools import chain\n",
    "    import pandas as pd  # if you need this, run 'pip install pandas==1.2.1'\n",
    "    from utils.dataset import ReflexiveDataset\n",
    "\n",
    "    sentences_splits = [ReflexiveDataset.sentences_from_tsv(sentences_dir, dataset_type, confidence_thrd,\n",
    "                                                            use_context=True)\n",
    "                        for dataset_type in [\"train\", \"val\", \"test\"]]\n",
    "    out_vertical = pd.DataFrame.from_records([s.to_dict() for s in chain(*sentences_splits)])\n",
    "    return out_vertical\n",
    "\n",
    "\n",
    "def split_text_to_sentence_context(text: str, sep_chars: Tuple[str] = (\".\", \"?\", \"!\")) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Splits the input text to sentences with the corresponding context,\n",
    "    in the format compliant with the training of NeuralClassifier\n",
    "    :param text: Full input paragraph, e.g. whole reflective diary, to extract the sentences to classify\n",
    "    :param sep_chars: characters separating potential sentences\n",
    "    \"\"\"\n",
    "    out_sentences = []\n",
    "    current_sent = []\n",
    "    words = text.split()\n",
    "\n",
    "    for w_i, word in enumerate(words):\n",
    "        current_sent.append(word)\n",
    "        is_last_or_is_upper = (w_i == len(words)-1 or words[w_i+1][0].isupper())\n",
    "        if any(word.endswith(mark) for mark in sep_chars) and is_last_or_is_upper:\n",
    "            out_sentences.append(\" \".join(current_sent))\n",
    "            current_sent = []\n",
    "\n",
    "    for sent_i, sent in enumerate(out_sentences):\n",
    "        context = \" \".join(out_sentences[sent_i-2:sent_i+2])\n",
    "        yield sent, context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7e10ff",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d2c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import List, Union, Dict\n",
    "import ast\n",
    "\n",
    "from filelock import FileLock\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer, InputFeatures, logging\n",
    "import pandas as pd\n",
    "# from .data_reader import Sentence\n",
    "\n",
    "logger = logging.get_logger()\n",
    "\n",
    "# original labels:\n",
    "# LABELS = [\"O\", \"OS_PRES\", \"PERS\", \"POC\", \"P_ZK\", \"REF_P\", \"UV_OBT\", \"VY_IN\", \"VY_VY\"]\n",
    "\n",
    "LABELS = [\"Other\", \"Belief\", \"Perspective\", \"Feeling\", \"Experience\",\n",
    "          \"Reflection\", \"Difficulty\", \"Intention\", \"Learning\"]\n",
    "\n",
    "\n",
    "class Split(Enum):\n",
    "    train = \"train\"\n",
    "    eval = \"eval\"\n",
    "    test = \"test\"\n",
    "\n",
    "\n",
    "class ReflexiveDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, sentences_dir: str, dataset_type: str, cache_dir: str, label_list: List[str],\n",
    "                 tokenizer: Union[AutoTokenizer, PreTrainedTokenizer],\n",
    "                 use_context=True, mean_confidence_threshold: int = 5):\n",
    "        self.sentences_dir = sentences_dir\n",
    "        self.confidence_thrd = mean_confidence_threshold\n",
    "        self.dataset_type = dataset_type\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_list = label_list\n",
    "        self.label_map = {label: i for i, label in enumerate(label_list)}\n",
    "        self.use_context = use_context\n",
    "\n",
    "        if not Path(cache_dir).exists():\n",
    "            Path(cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        cached_features_file = os.path.join(\n",
    "            cache_dir,\n",
    "            \"cached_{}_{}_{}\".format(dataset_type, tokenizer.__class__.__name__, str(self.tokenizer.model_max_length)),\n",
    "        )\n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "            logger.info(f\"Creating features from reflexive diaries\")\n",
    "            self.sentences = self.sentences_from_tsv(sentences_dir, dataset_type, self.confidence_thrd, self.use_context)\n",
    "            self.features = self.convert_examples_to_features(self.sentences)\n",
    "\n",
    "    @staticmethod\n",
    "    def sentences_from_tsv(sentences_dir: str, dataset_type: str,\n",
    "                           confidence_thrd: int, use_context: bool) -> List[Sentence]:\n",
    "        \"\"\"Creates sentences for the training, eval and test sets.\"\"\"\n",
    "        tsv_path = os.path.join(sentences_dir, dataset_type, \"sentences.tsv\")\n",
    "        df = pd.read_csv(tsv_path, sep='\\t')\n",
    "        df.sentence = df.sentence.fillna(\"\")\n",
    "        df.context = df.context.fillna(\"\")\n",
    "        sentences = []\n",
    "        # group by sources, iterate every group separately, to avoid context overlays\n",
    "        for idx, row in enumerate(df.itertuples()):\n",
    "            confidences = ast.literal_eval(row.confidence)\n",
    "            if sum(confidences) / len(confidences) >= confidence_thrd:\n",
    "                sentences.append(Sentence(id=row.idx, text=row.sentence,\n",
    "                                          context=row.context if use_context else None,\n",
    "                                          label=row.y))\n",
    "        logger.info(\"Retrieving %s of all %s %s sentences, over threshold %s\" %\n",
    "                    (len(sentences), len(df), dataset_type, confidence_thrd))\n",
    "        return sentences\n",
    "    \n",
    "    def convert_examples_to_features(self, examples: List[Sentence]) -> List[Dict[str, List[int]]]:\n",
    "\n",
    "        batch_encoding = self.tokenizer(\n",
    "            text=[example.text.strip() for example in examples],\n",
    "            text_pair=[e.context.strip() for e in examples] if self.use_context else None,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        features = []\n",
    "        for i in range(len(examples)):\n",
    "            inputs = {k: batch_encoding[k][i] for k in batch_encoding if k != \"token_type_ids\"}\n",
    "            inputs[\"label\"] = self.label_map[examples[i].label]\n",
    "            features.append(inputs)\n",
    "\n",
    "        for i, example in enumerate(examples[:5]):\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"id: %s\" % (example.id))\n",
    "            logger.info(\"features: %s\" % features[i])\n",
    "\n",
    "        return features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i) -> InputFeatures:\n",
    "        return self.features[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eac5743",
   "metadata": {},
   "source": [
    "# shallow_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bccbde54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from gensim import corpora\n",
    "from gensim import matutils\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#from .utils.dataset import ReflexiveDataset\n",
    "#from .utils.data_reader import Sentence\n",
    "\n",
    "\n",
    "class ShallowClassifier:\n",
    "    word_dictionary = None\n",
    "\n",
    "    def __init__(self, classifier, use_context: bool, bow_size: int,\n",
    "                 sentences_dir: Optional[str] = None, lang: str = \"cze\"):\n",
    "        self.classifier = classifier\n",
    "        self.sentences_dir = sentences_dir\n",
    "        self.use_context = use_context\n",
    "        self.bow_size = bow_size\n",
    "        self.lang = lang\n",
    "\n",
    "    def _preprocess_string(self, text: str) -> List[str]:\n",
    "        from gensim.parsing import preprocess_string\n",
    "        if self.lang == \"cze\":\n",
    "            from utils.cs_stemmer import cz_stem\n",
    "            return [cz_stem(word) for word in preprocess_string(text)]\n",
    "        else:\n",
    "            return preprocess_string(text)\n",
    "\n",
    "    def _initialize_bow_model(self, sents: List[Sentence]):\n",
    "        text_preprocessed = [self._preprocess_string(str(s.text)) for s in sents]\n",
    "        contexts_preprocessed = [self._preprocess_string(str(s.context)) for s in sents]\n",
    "\n",
    "        self.word_dictionary = corpora.Dictionary(text_preprocessed + contexts_preprocessed)\n",
    "        # keep most-occurring 10k words\n",
    "        # we need to check this with Ullmann\n",
    "        self.word_dictionary.filter_extremes(keep_n=self.bow_size)\n",
    "\n",
    "    def _vectorize_sentences(self, sents: List[Sentence]):\n",
    "        text_preprocessed = [self._preprocess_string(s.text) for s in sents]\n",
    "        # sparse matrix contains just pairs of co-occurrences\n",
    "        sparse_matrix = [self.word_dictionary.doc2bow(t) for t in text_preprocessed]\n",
    "        # we want to get natural, dense vectors for each document, containing the most-frequent num_terms\n",
    "        dense_matrix = matutils.corpus2dense(sparse_matrix, num_terms=self.bow_size).transpose()\n",
    "        if not self.use_context:\n",
    "            return dense_matrix\n",
    "        else:\n",
    "            # the same for contextual vectors\n",
    "            text_preprocessed_c = [self._preprocess_string(str(s.context)) for s in sents]\n",
    "            # sparse matrix contains just pairs of co-occurrences\n",
    "            sparse_matrix_c = [self.word_dictionary.doc2bow(t) for t in text_preprocessed_c]\n",
    "            # we want to get natural, dense vectors for each document, containing the most-frequent num_terms\n",
    "            dense_matrix_c = matutils.corpus2dense(sparse_matrix_c, num_terms=self.bow_size).transpose()\n",
    "\n",
    "            # concat textual and contextual vectors horizontally\n",
    "            return np.hstack([dense_matrix, dense_matrix_c])\n",
    "\n",
    "    def train(self, in_sentences: List[Sentence] = None, confidence_thrd: int = 5):\n",
    "        if in_sentences is None:\n",
    "            # get the dataset from outside\n",
    "            sentences = ReflexiveDataset.sentences_from_tsv(self.sentences_dir, \"train\",\n",
    "                                                            confidence_thrd, self.use_context)\n",
    "        else:\n",
    "            # user gets the dataset himself\n",
    "            sentences = in_sentences\n",
    "\n",
    "        self._initialize_bow_model(sentences)\n",
    "        vectors = self._vectorize_sentences(sentences)\n",
    "        self.classifier.fit(vectors, [s.label for s in in_sentences])\n",
    "\n",
    "    def predict(self, sentences: List[Sentence]):\n",
    "        vectors = self._vectorize_sentences(sentences)\n",
    "        targets = self.classifier.predict(vectors)\n",
    "        return targets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f5f770",
   "metadata": {},
   "source": [
    "# train_eval_shallow_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "227d3281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 149 sentences\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Belief       0.00      0.00      0.00         5\n",
      "Difficulties       0.00      0.00      0.00         6\n",
      "  Difficulty       0.00      0.00      0.00         0\n",
      "  Experience       0.67      0.13      0.22        31\n",
      "     Feeling       0.91      0.79      0.85        38\n",
      "    Learning       0.00      0.00      0.00         4\n",
      "       Other       0.52      0.92      0.67        59\n",
      "  Reflection       0.80      0.67      0.73         6\n",
      "\n",
      "    accuracy                           0.62       149\n",
      "   macro avg       0.36      0.31      0.31       149\n",
      "weighted avg       0.61      0.62      0.55       149\n",
      "\n",
      "0.6174496644295302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#from reflection_classification.utils.dataset import ReflexiveDataset\n",
    "#from reflection_classification.shallow_classifier import ShallowClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "#import argparse\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 하이퍼 파라미터 \n",
    "    classifier = 'random_forrest' # 분류기 선택 \n",
    "    sentences_dir = 'reflection-classification/data/sentences/en' # 데이터 위치 \n",
    "    train_confidence_threshold = 5\n",
    "    test_confidence_threshold = 5\n",
    "    use_context = True\n",
    "    vocabulary_size = 800\n",
    "    language ='en'\n",
    "    \n",
    "    # 분류기 설정 \n",
    "    if classifier == 'random_forrest':\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        classifier = RandomForestClassifier()\n",
    "    elif classifier == 'logistic_regression':\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        classifier = LogisticRegression(max_iter=10e4)\n",
    "    elif classifier == 'naive_bayes':\n",
    "        from sklearn.naive_bayes import MultinomialNB\n",
    "        classifier = MultinomialNB()\n",
    "    elif classifier == 'support_vector_classifier':\n",
    "        from sklearn.svm import SVC\n",
    "        classifier = SVC()\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized classifier: %s\" % classifier)\n",
    "\n",
    "    train_sentences = ReflexiveDataset.sentences_from_tsv(sentences_dir, \"train\",\n",
    "                                                          train_confidence_threshold, use_context)\n",
    "    test_sentences = ReflexiveDataset.sentences_from_tsv(sentences_dir, \"test\",\n",
    "                                                         test_confidence_threshold, use_context)\n",
    "\n",
    "    cfr = ShallowClassifier(classifier=classifier, use_context=use_context, bow_size=vocabulary_size,\n",
    "                            lang=language)\n",
    "    cfr.train(train_sentences)\n",
    "    pred_targets = cfr.predict(test_sentences)\n",
    "    true_targets = [s.label for s in test_sentences]\n",
    "    objective_val = f1_score(true_targets, pred_targets, average='micro')\n",
    "    print(\"Evaluating on %s sentences\" % len(test_sentences))\n",
    "    print(\"Classification report: \\n%s\" % classification_report(true_targets, pred_targets))\n",
    "    print(objective_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c917a5",
   "metadata": {},
   "source": [
    "# train_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f11f0179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from .dataset import ReflexiveDataset\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import EvalPrediction, AutoTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def eval_fscore_acc(p: EvalPrediction) -> Dict:\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    acc = (preds == p.label_ids).mean()\n",
    "    f1 = f1_score(y_true=p.label_ids, y_pred=preds, average='micro')\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"acc_and_f1\": (acc + f1) / 2,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_datasets(tokenizer: AutoTokenizer, sentences_dir: str, label_list: List[str],\n",
    "                 cache_dir: str, use_context: bool, confidence_thrd: int) -> List[ReflexiveDataset]:\n",
    "    return [ReflexiveDataset(sentences_dir, tokenizer=tokenizer, dataset_type=dataset_type, use_context=use_context,\n",
    "                             cache_dir=cache_dir, label_list=label_list, mean_confidence_threshold=confidence_thrd)\n",
    "            for dataset_type in [\"train\", \"val\", \"test\"]]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/transformer_config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d006a7",
   "metadata": {},
   "source": [
    "# train_neural_classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73759b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 59\u001b[0m\n\u001b[0;32m     49\u001b[0m transformer_config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     50\u001b[0m     model_args\u001b[38;5;241m.\u001b[39mmodel_name_or_path,\n\u001b[0;32m     51\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(LABELS),\n\u001b[0;32m     52\u001b[0m     finetuning_task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     54\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_args\u001b[38;5;241m.\u001b[39mmodel_name_or_path, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     56\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m---> 59\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m train_dataset, val_dataset, test_dataset \u001b[38;5;241m=\u001b[39m get_datasets(tokenizer,\n\u001b[0;32m     62\u001b[0m                                                         cache_dir\u001b[38;5;241m=\u001b[39mtrained_model_dir,\n\u001b[0;32m     63\u001b[0m                                                         label_list\u001b[38;5;241m=\u001b[39mLABELS,\n\u001b[0;32m     64\u001b[0m                                                         sentences_dir\u001b[38;5;241m=\u001b[39msentences_dir,\n\u001b[0;32m     65\u001b[0m                                                         use_context\u001b[38;5;241m=\u001b[39muse_context,\n\u001b[0;32m     66\u001b[0m                                                         confidence_thrd\u001b[38;5;241m=\u001b[39mtrain_confidence_threshold)\n\u001b[0;32m     67\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(output_dir\u001b[38;5;241m=\u001b[39mtrained_model_dir,\n\u001b[0;32m     68\u001b[0m                                   overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     69\u001b[0m                                   do_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m                                   no_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     84\u001b[0m                                   metric_for_best_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:2271\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   2267\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2268\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2269\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2270\u001b[0m         )\n\u001b[1;32m-> 2271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m     )\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, Trainer\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "#from reflection_classification.utils.train_utils import *\n",
    "#from reflection_classification.utils.dataset import LABELS\n",
    "\n",
    "# en_gtranslate thrd 5\n",
    "# Test accuracy: 0.7911392405063291\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # run this from /reflection-classification/reflection_classification\n",
    "    argparser = argparse.ArgumentParser()\n",
    "    \n",
    "    model_name = 'bert-base-multilingual-cased'\n",
    "    sentences_dir = 'reflection-classification/data/sentences/en'\n",
    "    trained_model_dir = 'reflection-classification/models/bert-base-en-confidence6+ \\ '\n",
    "    train_confidence_threshold=6\n",
    "    device = 'cuda' # cuda 환경을 만들어야 함 \n",
    "    eval_on_test_set = True\n",
    "    \n",
    "    argparser.add_argument('--model_name', type=str, help='Model name, or local path to finetune.',\n",
    "                           required=True)\n",
    "    argparser.add_argument('--sentences_dir', type=str, help='Directory with .tsvs of annotated sentences',\n",
    "                           required=True)\n",
    "    argparser.add_argument('--train_confidence_threshold', type=int,\n",
    "                           help='Minimal confidence threshold for sentences to train on.',\n",
    "                           default=5)\n",
    "    argparser.add_argument('--trained_model_dir', type=str, help='Directory to be filled with trained model',\n",
    "                           required=True)\n",
    "    argparser.add_argument('--device', type=str, help='Device used for training. One of {cpu, cuda, cuda:[idx]}',\n",
    "                           required=True, default=\"cuda\")\n",
    "    argparser.add_argument('--eval_on_test_set', type=bool, default=True,\n",
    "                           help='Whether to evaluate model (having lowest eval loss) on test set')\n",
    "    argparser.add_argument('--use_context', type=bool, help='Whether the model will be trained using context.',\n",
    "                           default=True)\n",
    "    \n",
    "\n",
    "    model_args = ModelArguments(\n",
    "        model_name_or_path=model_name,\n",
    "    )\n",
    "\n",
    "    transformer_config = AutoConfig.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        num_labels=len(LABELS),\n",
    "        finetuning_task=\"classification\",\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, use_fast=False)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        config=transformer_config,\n",
    "    ).to(device)\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = get_datasets(tokenizer,\n",
    "                                                            cache_dir=trained_model_dir,\n",
    "                                                            label_list=LABELS,\n",
    "                                                            sentences_dir=sentences_dir,\n",
    "                                                            use_context=use_context,\n",
    "                                                            confidence_thrd=train_confidence_threshold)\n",
    "    training_args = TrainingArguments(output_dir=trained_model_dir,\n",
    "                                      overwrite_output_dir=True,\n",
    "                                      do_train=True,\n",
    "                                      do_eval=True,\n",
    "                                      do_predict=True,\n",
    "                                      per_device_train_batch_size=2,\n",
    "                                      per_device_eval_batch_size=2,\n",
    "                                      num_train_epochs=20,\n",
    "                                      warmup_steps=300,\n",
    "                                      logging_steps=50,\n",
    "                                      logging_first_step=True,\n",
    "                                      evaluation_strategy=\"steps\",\n",
    "                                      learning_rate=2e-5,\n",
    "                                      save_total_limit=16,\n",
    "                                      gradient_accumulation_steps=16,\n",
    "                                      load_best_model_at_end=True,\n",
    "                                      no_cuda=True if device == \"cpu\" else False,\n",
    "                                      metric_for_best_model=\"f1\")\n",
    "\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    trainer = Trainer(model=model,\n",
    "                      args=training_args,\n",
    "                      train_dataset=train_dataset,\n",
    "                      eval_dataset=val_dataset,\n",
    "                      compute_metrics=eval_fscore_acc,\n",
    "                      tokenizer=tokenizer,\n",
    "                      callbacks=[EarlyStoppingCallback(early_stopping_patience=10)])\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    if eval_on_test_set:\n",
    "        y_pred = [trainer.model(\n",
    "            **{k: torch.tensor(v).unsqueeze(0).to(trainer.model.device) for k, v in f.items() if k != 'label'},\n",
    "            return_dict=True).logits.argmax().item() for f in tqdm(test_dataset.features,\n",
    "                                                                   desc=\"Evaluating best model on test dataset\")]\n",
    "\n",
    "        y_trues = [f['label'] for f in test_dataset.features]\n",
    "\n",
    "        y_truepos = [y_trues[i] == y_pred[i] for i, _ in enumerate(y_pred)]\n",
    "\n",
    "        print(\"Test accuracy: %s\" % (sum(y_truepos) / len(y_truepos)))\n",
    "\n",
    "    trainer.save_model(trained_model_dir)\n",
    "    print(\"Trained model and training checkpoints are saved to %s\" % trained_model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc76655",
   "metadata": {},
   "source": [
    "# neural_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "392b14fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "#from .utils.dataset import LABELS\n",
    "\n",
    "\n",
    "class NeuralClassifier:\n",
    "\n",
    "    def __init__(self, model_path: str, uses_context: bool, device: str):\n",
    "        self.config = AutoConfig.from_pretrained(model_path)\n",
    "        self.device = device\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path, config=self.config).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.uses_context = uses_context\n",
    "\n",
    "    def predict_sentence(self, sentence: str, context: str = None):\n",
    "        if context is None and self.uses_context:\n",
    "            raise ValueError(\"You need to pass in context argument, including the sentence\")\n",
    "\n",
    "        features = self.tokenizer(sentence, text_pair=context,\n",
    "                                  padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "        outputs = self.model(**features.to(self.device), return_dict=True)\n",
    "        argmax = outputs.logits.argmax(dim=-1).detach().cpu().tolist()[0]\n",
    "        labels = LABELS[argmax]\n",
    "\n",
    "        return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26133b",
   "metadata": {},
   "source": [
    "# eval_neural_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4db3019",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     14\u001b[0m use_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mNeuralClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m test_sentences \u001b[38;5;241m=\u001b[39m ReflexiveDataset\u001b[38;5;241m.\u001b[39msentences_from_tsv(sentences_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m                                                      test_confidence_threshold, use_context)\n\u001b[0;32m     21\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m [classifier\u001b[38;5;241m.\u001b[39mpredict_sentence(sentence\u001b[38;5;241m.\u001b[39mtext, sentence\u001b[38;5;241m.\u001b[39mcontext) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m tqdm(test_sentences)]\n",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m, in \u001b[0;36mNeuralClassifier.__init__\u001b[1;34m(self, model_path, uses_context, device)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muses_context \u001b[38;5;241m=\u001b[39m uses_context\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:2271\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   2267\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2268\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2269\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2270\u001b[0m         )\n\u001b[1;32m-> 2271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m     )\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from reflection_classification.utils.dataset import ReflexiveDataset\n",
    "#from reflection_classification.neural_classifier import NeuralClassifier\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #argparser = argparse.ArgumentParser()\n",
    "    \n",
    "    sentences_dir='reflection-classification/data/sentences/en'\n",
    "    trained_model_dir='reflection-classification/models/roberta-large-nouda'\n",
    "    test_confidence_threshold=5\n",
    "    device = 'cuda'\n",
    "    use_context = True\n",
    "    \n",
    "\n",
    "    classifier = NeuralClassifier(trained_model_dir, use_context, device)\n",
    "    test_sentences = ReflexiveDataset.sentences_from_tsv(sentences_dir, \"test\",\n",
    "                                                         test_confidence_threshold, use_context)\n",
    "\n",
    "    y_pred = [classifier.predict_sentence(sentence.text, sentence.context) for sentence in tqdm(test_sentences)]\n",
    "\n",
    "    y_trues = [sentence.label for sentence in test_sentences]\n",
    "\n",
    "    y_truepos = [y_trues[i] == y_pred[i] for i, _ in enumerate(y_pred)]\n",
    "\n",
    "    print(\"Test accuracy: %s\" % (sum(y_truepos) / len(y_truepos)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2736b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6299aa32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
